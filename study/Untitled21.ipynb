{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(range(1,101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_1_slice(dataset,start,x,y):\n",
    "    data = []\n",
    "    for i in range(start-1,len(dataset)-x-y+1):\n",
    "        data.append(dataset[i:i+x+y])\n",
    "    data = np.array(data)\n",
    "    \n",
    "    x_data = data[:,0:x].reshape(-1,7,1)\n",
    "    y_data = data[:,x:x+y]\n",
    "    \n",
    "    return x_data,y_data\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 1],\n",
       "         [ 2],\n",
       "         [ 3],\n",
       "         [ 4],\n",
       "         [ 5],\n",
       "         [ 6],\n",
       "         [ 7]],\n",
       " \n",
       "        [[ 2],\n",
       "         [ 3],\n",
       "         [ 4],\n",
       "         [ 5],\n",
       "         [ 6],\n",
       "         [ 7],\n",
       "         [ 8]],\n",
       " \n",
       "        [[ 3],\n",
       "         [ 4],\n",
       "         [ 5],\n",
       "         [ 6],\n",
       "         [ 7],\n",
       "         [ 8],\n",
       "         [ 9]],\n",
       " \n",
       "        [[ 4],\n",
       "         [ 5],\n",
       "         [ 6],\n",
       "         [ 7],\n",
       "         [ 8],\n",
       "         [ 9],\n",
       "         [10]],\n",
       " \n",
       "        [[ 5],\n",
       "         [ 6],\n",
       "         [ 7],\n",
       "         [ 8],\n",
       "         [ 9],\n",
       "         [10],\n",
       "         [11]],\n",
       " \n",
       "        [[ 6],\n",
       "         [ 7],\n",
       "         [ 8],\n",
       "         [ 9],\n",
       "         [10],\n",
       "         [11],\n",
       "         [12]],\n",
       " \n",
       "        [[ 7],\n",
       "         [ 8],\n",
       "         [ 9],\n",
       "         [10],\n",
       "         [11],\n",
       "         [12],\n",
       "         [13]],\n",
       " \n",
       "        [[ 8],\n",
       "         [ 9],\n",
       "         [10],\n",
       "         [11],\n",
       "         [12],\n",
       "         [13],\n",
       "         [14]],\n",
       " \n",
       "        [[ 9],\n",
       "         [10],\n",
       "         [11],\n",
       "         [12],\n",
       "         [13],\n",
       "         [14],\n",
       "         [15]],\n",
       " \n",
       "        [[10],\n",
       "         [11],\n",
       "         [12],\n",
       "         [13],\n",
       "         [14],\n",
       "         [15],\n",
       "         [16]],\n",
       " \n",
       "        [[11],\n",
       "         [12],\n",
       "         [13],\n",
       "         [14],\n",
       "         [15],\n",
       "         [16],\n",
       "         [17]],\n",
       " \n",
       "        [[12],\n",
       "         [13],\n",
       "         [14],\n",
       "         [15],\n",
       "         [16],\n",
       "         [17],\n",
       "         [18]],\n",
       " \n",
       "        [[13],\n",
       "         [14],\n",
       "         [15],\n",
       "         [16],\n",
       "         [17],\n",
       "         [18],\n",
       "         [19]],\n",
       " \n",
       "        [[14],\n",
       "         [15],\n",
       "         [16],\n",
       "         [17],\n",
       "         [18],\n",
       "         [19],\n",
       "         [20]],\n",
       " \n",
       "        [[15],\n",
       "         [16],\n",
       "         [17],\n",
       "         [18],\n",
       "         [19],\n",
       "         [20],\n",
       "         [21]],\n",
       " \n",
       "        [[16],\n",
       "         [17],\n",
       "         [18],\n",
       "         [19],\n",
       "         [20],\n",
       "         [21],\n",
       "         [22]],\n",
       " \n",
       "        [[17],\n",
       "         [18],\n",
       "         [19],\n",
       "         [20],\n",
       "         [21],\n",
       "         [22],\n",
       "         [23]],\n",
       " \n",
       "        [[18],\n",
       "         [19],\n",
       "         [20],\n",
       "         [21],\n",
       "         [22],\n",
       "         [23],\n",
       "         [24]],\n",
       " \n",
       "        [[19],\n",
       "         [20],\n",
       "         [21],\n",
       "         [22],\n",
       "         [23],\n",
       "         [24],\n",
       "         [25]],\n",
       " \n",
       "        [[20],\n",
       "         [21],\n",
       "         [22],\n",
       "         [23],\n",
       "         [24],\n",
       "         [25],\n",
       "         [26]],\n",
       " \n",
       "        [[21],\n",
       "         [22],\n",
       "         [23],\n",
       "         [24],\n",
       "         [25],\n",
       "         [26],\n",
       "         [27]],\n",
       " \n",
       "        [[22],\n",
       "         [23],\n",
       "         [24],\n",
       "         [25],\n",
       "         [26],\n",
       "         [27],\n",
       "         [28]],\n",
       " \n",
       "        [[23],\n",
       "         [24],\n",
       "         [25],\n",
       "         [26],\n",
       "         [27],\n",
       "         [28],\n",
       "         [29]],\n",
       " \n",
       "        [[24],\n",
       "         [25],\n",
       "         [26],\n",
       "         [27],\n",
       "         [28],\n",
       "         [29],\n",
       "         [30]],\n",
       " \n",
       "        [[25],\n",
       "         [26],\n",
       "         [27],\n",
       "         [28],\n",
       "         [29],\n",
       "         [30],\n",
       "         [31]],\n",
       " \n",
       "        [[26],\n",
       "         [27],\n",
       "         [28],\n",
       "         [29],\n",
       "         [30],\n",
       "         [31],\n",
       "         [32]],\n",
       " \n",
       "        [[27],\n",
       "         [28],\n",
       "         [29],\n",
       "         [30],\n",
       "         [31],\n",
       "         [32],\n",
       "         [33]],\n",
       " \n",
       "        [[28],\n",
       "         [29],\n",
       "         [30],\n",
       "         [31],\n",
       "         [32],\n",
       "         [33],\n",
       "         [34]],\n",
       " \n",
       "        [[29],\n",
       "         [30],\n",
       "         [31],\n",
       "         [32],\n",
       "         [33],\n",
       "         [34],\n",
       "         [35]],\n",
       " \n",
       "        [[30],\n",
       "         [31],\n",
       "         [32],\n",
       "         [33],\n",
       "         [34],\n",
       "         [35],\n",
       "         [36]],\n",
       " \n",
       "        [[31],\n",
       "         [32],\n",
       "         [33],\n",
       "         [34],\n",
       "         [35],\n",
       "         [36],\n",
       "         [37]],\n",
       " \n",
       "        [[32],\n",
       "         [33],\n",
       "         [34],\n",
       "         [35],\n",
       "         [36],\n",
       "         [37],\n",
       "         [38]],\n",
       " \n",
       "        [[33],\n",
       "         [34],\n",
       "         [35],\n",
       "         [36],\n",
       "         [37],\n",
       "         [38],\n",
       "         [39]],\n",
       " \n",
       "        [[34],\n",
       "         [35],\n",
       "         [36],\n",
       "         [37],\n",
       "         [38],\n",
       "         [39],\n",
       "         [40]],\n",
       " \n",
       "        [[35],\n",
       "         [36],\n",
       "         [37],\n",
       "         [38],\n",
       "         [39],\n",
       "         [40],\n",
       "         [41]],\n",
       " \n",
       "        [[36],\n",
       "         [37],\n",
       "         [38],\n",
       "         [39],\n",
       "         [40],\n",
       "         [41],\n",
       "         [42]],\n",
       " \n",
       "        [[37],\n",
       "         [38],\n",
       "         [39],\n",
       "         [40],\n",
       "         [41],\n",
       "         [42],\n",
       "         [43]],\n",
       " \n",
       "        [[38],\n",
       "         [39],\n",
       "         [40],\n",
       "         [41],\n",
       "         [42],\n",
       "         [43],\n",
       "         [44]],\n",
       " \n",
       "        [[39],\n",
       "         [40],\n",
       "         [41],\n",
       "         [42],\n",
       "         [43],\n",
       "         [44],\n",
       "         [45]],\n",
       " \n",
       "        [[40],\n",
       "         [41],\n",
       "         [42],\n",
       "         [43],\n",
       "         [44],\n",
       "         [45],\n",
       "         [46]],\n",
       " \n",
       "        [[41],\n",
       "         [42],\n",
       "         [43],\n",
       "         [44],\n",
       "         [45],\n",
       "         [46],\n",
       "         [47]],\n",
       " \n",
       "        [[42],\n",
       "         [43],\n",
       "         [44],\n",
       "         [45],\n",
       "         [46],\n",
       "         [47],\n",
       "         [48]],\n",
       " \n",
       "        [[43],\n",
       "         [44],\n",
       "         [45],\n",
       "         [46],\n",
       "         [47],\n",
       "         [48],\n",
       "         [49]],\n",
       " \n",
       "        [[44],\n",
       "         [45],\n",
       "         [46],\n",
       "         [47],\n",
       "         [48],\n",
       "         [49],\n",
       "         [50]],\n",
       " \n",
       "        [[45],\n",
       "         [46],\n",
       "         [47],\n",
       "         [48],\n",
       "         [49],\n",
       "         [50],\n",
       "         [51]],\n",
       " \n",
       "        [[46],\n",
       "         [47],\n",
       "         [48],\n",
       "         [49],\n",
       "         [50],\n",
       "         [51],\n",
       "         [52]],\n",
       " \n",
       "        [[47],\n",
       "         [48],\n",
       "         [49],\n",
       "         [50],\n",
       "         [51],\n",
       "         [52],\n",
       "         [53]],\n",
       " \n",
       "        [[48],\n",
       "         [49],\n",
       "         [50],\n",
       "         [51],\n",
       "         [52],\n",
       "         [53],\n",
       "         [54]],\n",
       " \n",
       "        [[49],\n",
       "         [50],\n",
       "         [51],\n",
       "         [52],\n",
       "         [53],\n",
       "         [54],\n",
       "         [55]],\n",
       " \n",
       "        [[50],\n",
       "         [51],\n",
       "         [52],\n",
       "         [53],\n",
       "         [54],\n",
       "         [55],\n",
       "         [56]],\n",
       " \n",
       "        [[51],\n",
       "         [52],\n",
       "         [53],\n",
       "         [54],\n",
       "         [55],\n",
       "         [56],\n",
       "         [57]],\n",
       " \n",
       "        [[52],\n",
       "         [53],\n",
       "         [54],\n",
       "         [55],\n",
       "         [56],\n",
       "         [57],\n",
       "         [58]],\n",
       " \n",
       "        [[53],\n",
       "         [54],\n",
       "         [55],\n",
       "         [56],\n",
       "         [57],\n",
       "         [58],\n",
       "         [59]],\n",
       " \n",
       "        [[54],\n",
       "         [55],\n",
       "         [56],\n",
       "         [57],\n",
       "         [58],\n",
       "         [59],\n",
       "         [60]],\n",
       " \n",
       "        [[55],\n",
       "         [56],\n",
       "         [57],\n",
       "         [58],\n",
       "         [59],\n",
       "         [60],\n",
       "         [61]],\n",
       " \n",
       "        [[56],\n",
       "         [57],\n",
       "         [58],\n",
       "         [59],\n",
       "         [60],\n",
       "         [61],\n",
       "         [62]],\n",
       " \n",
       "        [[57],\n",
       "         [58],\n",
       "         [59],\n",
       "         [60],\n",
       "         [61],\n",
       "         [62],\n",
       "         [63]],\n",
       " \n",
       "        [[58],\n",
       "         [59],\n",
       "         [60],\n",
       "         [61],\n",
       "         [62],\n",
       "         [63],\n",
       "         [64]],\n",
       " \n",
       "        [[59],\n",
       "         [60],\n",
       "         [61],\n",
       "         [62],\n",
       "         [63],\n",
       "         [64],\n",
       "         [65]],\n",
       " \n",
       "        [[60],\n",
       "         [61],\n",
       "         [62],\n",
       "         [63],\n",
       "         [64],\n",
       "         [65],\n",
       "         [66]],\n",
       " \n",
       "        [[61],\n",
       "         [62],\n",
       "         [63],\n",
       "         [64],\n",
       "         [65],\n",
       "         [66],\n",
       "         [67]],\n",
       " \n",
       "        [[62],\n",
       "         [63],\n",
       "         [64],\n",
       "         [65],\n",
       "         [66],\n",
       "         [67],\n",
       "         [68]],\n",
       " \n",
       "        [[63],\n",
       "         [64],\n",
       "         [65],\n",
       "         [66],\n",
       "         [67],\n",
       "         [68],\n",
       "         [69]],\n",
       " \n",
       "        [[64],\n",
       "         [65],\n",
       "         [66],\n",
       "         [67],\n",
       "         [68],\n",
       "         [69],\n",
       "         [70]],\n",
       " \n",
       "        [[65],\n",
       "         [66],\n",
       "         [67],\n",
       "         [68],\n",
       "         [69],\n",
       "         [70],\n",
       "         [71]],\n",
       " \n",
       "        [[66],\n",
       "         [67],\n",
       "         [68],\n",
       "         [69],\n",
       "         [70],\n",
       "         [71],\n",
       "         [72]],\n",
       " \n",
       "        [[67],\n",
       "         [68],\n",
       "         [69],\n",
       "         [70],\n",
       "         [71],\n",
       "         [72],\n",
       "         [73]],\n",
       " \n",
       "        [[68],\n",
       "         [69],\n",
       "         [70],\n",
       "         [71],\n",
       "         [72],\n",
       "         [73],\n",
       "         [74]],\n",
       " \n",
       "        [[69],\n",
       "         [70],\n",
       "         [71],\n",
       "         [72],\n",
       "         [73],\n",
       "         [74],\n",
       "         [75]],\n",
       " \n",
       "        [[70],\n",
       "         [71],\n",
       "         [72],\n",
       "         [73],\n",
       "         [74],\n",
       "         [75],\n",
       "         [76]],\n",
       " \n",
       "        [[71],\n",
       "         [72],\n",
       "         [73],\n",
       "         [74],\n",
       "         [75],\n",
       "         [76],\n",
       "         [77]],\n",
       " \n",
       "        [[72],\n",
       "         [73],\n",
       "         [74],\n",
       "         [75],\n",
       "         [76],\n",
       "         [77],\n",
       "         [78]],\n",
       " \n",
       "        [[73],\n",
       "         [74],\n",
       "         [75],\n",
       "         [76],\n",
       "         [77],\n",
       "         [78],\n",
       "         [79]],\n",
       " \n",
       "        [[74],\n",
       "         [75],\n",
       "         [76],\n",
       "         [77],\n",
       "         [78],\n",
       "         [79],\n",
       "         [80]],\n",
       " \n",
       "        [[75],\n",
       "         [76],\n",
       "         [77],\n",
       "         [78],\n",
       "         [79],\n",
       "         [80],\n",
       "         [81]],\n",
       " \n",
       "        [[76],\n",
       "         [77],\n",
       "         [78],\n",
       "         [79],\n",
       "         [80],\n",
       "         [81],\n",
       "         [82]],\n",
       " \n",
       "        [[77],\n",
       "         [78],\n",
       "         [79],\n",
       "         [80],\n",
       "         [81],\n",
       "         [82],\n",
       "         [83]],\n",
       " \n",
       "        [[78],\n",
       "         [79],\n",
       "         [80],\n",
       "         [81],\n",
       "         [82],\n",
       "         [83],\n",
       "         [84]],\n",
       " \n",
       "        [[79],\n",
       "         [80],\n",
       "         [81],\n",
       "         [82],\n",
       "         [83],\n",
       "         [84],\n",
       "         [85]],\n",
       " \n",
       "        [[80],\n",
       "         [81],\n",
       "         [82],\n",
       "         [83],\n",
       "         [84],\n",
       "         [85],\n",
       "         [86]],\n",
       " \n",
       "        [[81],\n",
       "         [82],\n",
       "         [83],\n",
       "         [84],\n",
       "         [85],\n",
       "         [86],\n",
       "         [87]],\n",
       " \n",
       "        [[82],\n",
       "         [83],\n",
       "         [84],\n",
       "         [85],\n",
       "         [86],\n",
       "         [87],\n",
       "         [88]],\n",
       " \n",
       "        [[83],\n",
       "         [84],\n",
       "         [85],\n",
       "         [86],\n",
       "         [87],\n",
       "         [88],\n",
       "         [89]],\n",
       " \n",
       "        [[84],\n",
       "         [85],\n",
       "         [86],\n",
       "         [87],\n",
       "         [88],\n",
       "         [89],\n",
       "         [90]],\n",
       " \n",
       "        [[85],\n",
       "         [86],\n",
       "         [87],\n",
       "         [88],\n",
       "         [89],\n",
       "         [90],\n",
       "         [91]],\n",
       " \n",
       "        [[86],\n",
       "         [87],\n",
       "         [88],\n",
       "         [89],\n",
       "         [90],\n",
       "         [91],\n",
       "         [92]],\n",
       " \n",
       "        [[87],\n",
       "         [88],\n",
       "         [89],\n",
       "         [90],\n",
       "         [91],\n",
       "         [92],\n",
       "         [93]],\n",
       " \n",
       "        [[88],\n",
       "         [89],\n",
       "         [90],\n",
       "         [91],\n",
       "         [92],\n",
       "         [93],\n",
       "         [94]],\n",
       " \n",
       "        [[89],\n",
       "         [90],\n",
       "         [91],\n",
       "         [92],\n",
       "         [93],\n",
       "         [94],\n",
       "         [95]],\n",
       " \n",
       "        [[90],\n",
       "         [91],\n",
       "         [92],\n",
       "         [93],\n",
       "         [94],\n",
       "         [95],\n",
       "         [96]],\n",
       " \n",
       "        [[91],\n",
       "         [92],\n",
       "         [93],\n",
       "         [94],\n",
       "         [95],\n",
       "         [96],\n",
       "         [97]]]),\n",
       " array([[  8,   9,  10],\n",
       "        [  9,  10,  11],\n",
       "        [ 10,  11,  12],\n",
       "        [ 11,  12,  13],\n",
       "        [ 12,  13,  14],\n",
       "        [ 13,  14,  15],\n",
       "        [ 14,  15,  16],\n",
       "        [ 15,  16,  17],\n",
       "        [ 16,  17,  18],\n",
       "        [ 17,  18,  19],\n",
       "        [ 18,  19,  20],\n",
       "        [ 19,  20,  21],\n",
       "        [ 20,  21,  22],\n",
       "        [ 21,  22,  23],\n",
       "        [ 22,  23,  24],\n",
       "        [ 23,  24,  25],\n",
       "        [ 24,  25,  26],\n",
       "        [ 25,  26,  27],\n",
       "        [ 26,  27,  28],\n",
       "        [ 27,  28,  29],\n",
       "        [ 28,  29,  30],\n",
       "        [ 29,  30,  31],\n",
       "        [ 30,  31,  32],\n",
       "        [ 31,  32,  33],\n",
       "        [ 32,  33,  34],\n",
       "        [ 33,  34,  35],\n",
       "        [ 34,  35,  36],\n",
       "        [ 35,  36,  37],\n",
       "        [ 36,  37,  38],\n",
       "        [ 37,  38,  39],\n",
       "        [ 38,  39,  40],\n",
       "        [ 39,  40,  41],\n",
       "        [ 40,  41,  42],\n",
       "        [ 41,  42,  43],\n",
       "        [ 42,  43,  44],\n",
       "        [ 43,  44,  45],\n",
       "        [ 44,  45,  46],\n",
       "        [ 45,  46,  47],\n",
       "        [ 46,  47,  48],\n",
       "        [ 47,  48,  49],\n",
       "        [ 48,  49,  50],\n",
       "        [ 49,  50,  51],\n",
       "        [ 50,  51,  52],\n",
       "        [ 51,  52,  53],\n",
       "        [ 52,  53,  54],\n",
       "        [ 53,  54,  55],\n",
       "        [ 54,  55,  56],\n",
       "        [ 55,  56,  57],\n",
       "        [ 56,  57,  58],\n",
       "        [ 57,  58,  59],\n",
       "        [ 58,  59,  60],\n",
       "        [ 59,  60,  61],\n",
       "        [ 60,  61,  62],\n",
       "        [ 61,  62,  63],\n",
       "        [ 62,  63,  64],\n",
       "        [ 63,  64,  65],\n",
       "        [ 64,  65,  66],\n",
       "        [ 65,  66,  67],\n",
       "        [ 66,  67,  68],\n",
       "        [ 67,  68,  69],\n",
       "        [ 68,  69,  70],\n",
       "        [ 69,  70,  71],\n",
       "        [ 70,  71,  72],\n",
       "        [ 71,  72,  73],\n",
       "        [ 72,  73,  74],\n",
       "        [ 73,  74,  75],\n",
       "        [ 74,  75,  76],\n",
       "        [ 75,  76,  77],\n",
       "        [ 76,  77,  78],\n",
       "        [ 77,  78,  79],\n",
       "        [ 78,  79,  80],\n",
       "        [ 79,  80,  81],\n",
       "        [ 80,  81,  82],\n",
       "        [ 81,  82,  83],\n",
       "        [ 82,  83,  84],\n",
       "        [ 83,  84,  85],\n",
       "        [ 84,  85,  86],\n",
       "        [ 85,  86,  87],\n",
       "        [ 86,  87,  88],\n",
       "        [ 87,  88,  89],\n",
       "        [ 88,  89,  90],\n",
       "        [ 89,  90,  91],\n",
       "        [ 90,  91,  92],\n",
       "        [ 91,  92,  93],\n",
       "        [ 92,  93,  94],\n",
       "        [ 93,  94,  95],\n",
       "        [ 94,  95,  96],\n",
       "        [ 95,  96,  97],\n",
       "        [ 96,  97,  98],\n",
       "        [ 97,  98,  99],\n",
       "        [ 98,  99, 100]]))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1_slice(dataset,1,7,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[91]\n",
      "  [92]\n",
      "  [93]\n",
      "  [94]\n",
      "  [95]\n",
      "  [96]\n",
      "  [97]]]\n"
     ]
    }
   ],
   "source": [
    "x_hat, _ = test_1_slice(dataset,91,7,3)\n",
    "print(x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = test_1_slice(dataset,7,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, LSTM, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               40800     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 62,179\n",
      "Trainable params: 62,179\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 8.2257 - mse: 3257.7520 - val_loss: 6.1680 - val_mse: 3059.1379\n",
      "Epoch 2/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.3632 - mse: 2455.9087 - val_loss: 5.4794 - val_mse: 2383.6619\n",
      "Epoch 3/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.1019 - mse: 2197.6836 - val_loss: 5.4068 - val_mse: 2288.7922\n",
      "Epoch 4/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 5.0705 - mse: 2090.1086 - val_loss: 5.4062 - val_mse: 2215.7197\n",
      "Epoch 5/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.0794 - mse: 2033.9067 - val_loss: 5.4008 - val_mse: 2227.6626\n",
      "Epoch 6/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.0866 - mse: 2168.5427 - val_loss: 5.4055 - val_mse: 2078.8643\n",
      "Epoch 7/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 4.9646 - mse: 1712.9849 - val_loss: 5.3005 - val_mse: 1575.6094\n",
      "Epoch 8/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.6266 - mse: 1015.8818 - val_loss: 0.3299 - val_mse: 674.7402\n",
      "Epoch 9/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1687 - mse: 386.8154 - val_loss: 0.0292 - val_mse: 102.1758\n",
      "Epoch 10/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0551 - mse: 210.6504 - val_loss: 0.0110 - val_mse: 40.6312\n",
      "Epoch 11/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0561 - mse: 140.3709 - val_loss: 0.0150 - val_mse: 51.9086\n",
      "Epoch 12/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0470 - mse: 159.5417 - val_loss: 0.0125 - val_mse: 37.0272\n",
      "Epoch 13/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0524 - mse: 180.2839 - val_loss: 0.0062 - val_mse: 26.2837\n",
      "Epoch 14/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0389 - mse: 122.4105 - val_loss: 0.0141 - val_mse: 48.5606\n",
      "Epoch 15/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0422 - mse: 152.6224 - val_loss: 0.0033 - val_mse: 21.4285\n",
      "Epoch 16/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0422 - mse: 117.0562 - val_loss: 0.0252 - val_mse: 35.8432\n",
      "Epoch 17/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0423 - mse: 148.2518 - val_loss: 0.0024 - val_mse: 12.0032\n",
      "Epoch 18/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0435 - mse: 153.5138 - val_loss: 0.0226 - val_mse: 93.4260\n",
      "Epoch 19/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0353 - mse: 107.8023 - val_loss: 0.0157 - val_mse: 63.7546\n",
      "Epoch 20/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0494 - mse: 145.5895 - val_loss: 0.0116 - val_mse: 37.6964\n",
      "Epoch 21/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0284 - mse: 101.4657 - val_loss: 0.0022 - val_mse: 6.9146\n",
      "Epoch 22/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0352 - mse: 96.6190 - val_loss: 0.0013 - val_mse: 5.9585\n",
      "Epoch 23/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0325 - mse: 125.1615 - val_loss: 0.0023 - val_mse: 7.3261\n",
      "Epoch 24/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0243 - mse: 74.5070 - val_loss: 0.0699 - val_mse: 233.6365\n",
      "Epoch 25/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0234 - mse: 76.8319 - val_loss: 0.0017 - val_mse: 9.6859\n",
      "Epoch 26/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0387 - mse: 123.6718 - val_loss: 0.0077 - val_mse: 48.3344\n",
      "Epoch 27/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0228 - mse: 75.5824 - val_loss: 0.0057 - val_mse: 29.0734\n",
      "Epoch 28/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0451 - mse: 142.7639 - val_loss: 0.0069 - val_mse: 25.0227\n",
      "Epoch 29/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0359 - mse: 151.4252 - val_loss: 0.0041 - val_mse: 19.3747\n",
      "Epoch 30/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0283 - mse: 91.8547 - val_loss: 0.0100 - val_mse: 32.3404\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 23.8303\n",
      "test loss :    0.010370220057666302    mse   :  23.830318450927734\n",
      "y_predict :   [[97.12287 97.48949 98.55267]]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape = (7,1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss = 'msle', metrics = ['mse'])\n",
    "model.fit(x_train,y_train,epochs=30,validation_split=0.2,batch_size=1)\n",
    "\n",
    "loss, mse = model.evaluate(x_test,y_test)\n",
    "\n",
    "x_hat = np.array([[[111+i] for i in range(0,7)]])\n",
    "\n",
    "y_predict = model.predict(x_hat)\n",
    "\n",
    "\n",
    "print(f'test loss :    {loss}    mse   :  {mse}')\n",
    "print(f'y_predict :   {y_predict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(range(1,118))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2_slice(dataset,start,end, x_shape,y):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for i in range(start-1,end-1):\n",
    "        xlst=[]\n",
    "        ylst=[]\n",
    "        for j in range(x_shape[0]):\n",
    "            xlst.append(dataset[i+j:i+j+x_shape[1]])\n",
    "            ylst.append(dataset[i+j+x_shape[1]+2])\n",
    "        x_data.append(xlst)\n",
    "        y_data.append(ylst)\n",
    "    \n",
    "    return np.array(x_data).reshape(-1,x_shape[0],x_shape[1],1), np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 117 is out of bounds for axis 0 with size 117",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-191-e1a896756c00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest2_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m114\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-190-a6264cf0d5f5>\u001b[0m in \u001b[0;36mtest2_slice\u001b[1;34m(dataset, start, end, x_shape, y)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mxlst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mx_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mylst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mx_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mx_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxlst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0my_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mylst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 117 is out of bounds for axis 0 with size 117"
     ]
    }
   ],
   "source": [
    "a,b=test2_slice(dataset,1,114,(3,7),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[  1]\n",
      "   [  2]\n",
      "   [  3]\n",
      "   ...\n",
      "   [  5]\n",
      "   [  6]\n",
      "   [  7]]\n",
      "\n",
      "  [[  2]\n",
      "   [  3]\n",
      "   [  4]\n",
      "   ...\n",
      "   [  6]\n",
      "   [  7]\n",
      "   [  8]]\n",
      "\n",
      "  [[  3]\n",
      "   [  4]\n",
      "   [  5]\n",
      "   ...\n",
      "   [  7]\n",
      "   [  8]\n",
      "   [  9]]]\n",
      "\n",
      "\n",
      " [[[  2]\n",
      "   [  3]\n",
      "   [  4]\n",
      "   ...\n",
      "   [  6]\n",
      "   [  7]\n",
      "   [  8]]\n",
      "\n",
      "  [[  3]\n",
      "   [  4]\n",
      "   [  5]\n",
      "   ...\n",
      "   [  7]\n",
      "   [  8]\n",
      "   [  9]]\n",
      "\n",
      "  [[  4]\n",
      "   [  5]\n",
      "   [  6]\n",
      "   ...\n",
      "   [  8]\n",
      "   [  9]\n",
      "   [ 10]]]\n",
      "\n",
      "\n",
      " [[[  3]\n",
      "   [  4]\n",
      "   [  5]\n",
      "   ...\n",
      "   [  7]\n",
      "   [  8]\n",
      "   [  9]]\n",
      "\n",
      "  [[  4]\n",
      "   [  5]\n",
      "   [  6]\n",
      "   ...\n",
      "   [  8]\n",
      "   [  9]\n",
      "   [ 10]]\n",
      "\n",
      "  [[  5]\n",
      "   [  6]\n",
      "   [  7]\n",
      "   ...\n",
      "   [  9]\n",
      "   [ 10]\n",
      "   [ 11]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[104]\n",
      "   [105]\n",
      "   [106]\n",
      "   ...\n",
      "   [108]\n",
      "   [109]\n",
      "   [110]]\n",
      "\n",
      "  [[105]\n",
      "   [106]\n",
      "   [107]\n",
      "   ...\n",
      "   [109]\n",
      "   [110]\n",
      "   [111]]\n",
      "\n",
      "  [[106]\n",
      "   [107]\n",
      "   [108]\n",
      "   ...\n",
      "   [110]\n",
      "   [111]\n",
      "   [112]]]\n",
      "\n",
      "\n",
      " [[[105]\n",
      "   [106]\n",
      "   [107]\n",
      "   ...\n",
      "   [109]\n",
      "   [110]\n",
      "   [111]]\n",
      "\n",
      "  [[106]\n",
      "   [107]\n",
      "   [108]\n",
      "   ...\n",
      "   [110]\n",
      "   [111]\n",
      "   [112]]\n",
      "\n",
      "  [[107]\n",
      "   [108]\n",
      "   [109]\n",
      "   ...\n",
      "   [111]\n",
      "   [112]\n",
      "   [113]]]\n",
      "\n",
      "\n",
      " [[[106]\n",
      "   [107]\n",
      "   [108]\n",
      "   ...\n",
      "   [110]\n",
      "   [111]\n",
      "   [112]]\n",
      "\n",
      "  [[107]\n",
      "   [108]\n",
      "   [109]\n",
      "   ...\n",
      "   [111]\n",
      "   [112]\n",
      "   [113]]\n",
      "\n",
      "  [[108]\n",
      "   [109]\n",
      "   [110]\n",
      "   ...\n",
      "   [112]\n",
      "   [113]\n",
      "   [114]]]]\n",
      "[[ 10  11  12]\n",
      " [ 11  12  13]\n",
      " [ 12  13  14]\n",
      " [ 13  14  15]\n",
      " [ 14  15  16]\n",
      " [ 15  16  17]\n",
      " [ 16  17  18]\n",
      " [ 17  18  19]\n",
      " [ 18  19  20]\n",
      " [ 19  20  21]\n",
      " [ 20  21  22]\n",
      " [ 21  22  23]\n",
      " [ 22  23  24]\n",
      " [ 23  24  25]\n",
      " [ 24  25  26]\n",
      " [ 25  26  27]\n",
      " [ 26  27  28]\n",
      " [ 27  28  29]\n",
      " [ 28  29  30]\n",
      " [ 29  30  31]\n",
      " [ 30  31  32]\n",
      " [ 31  32  33]\n",
      " [ 32  33  34]\n",
      " [ 33  34  35]\n",
      " [ 34  35  36]\n",
      " [ 35  36  37]\n",
      " [ 36  37  38]\n",
      " [ 37  38  39]\n",
      " [ 38  39  40]\n",
      " [ 39  40  41]\n",
      " [ 40  41  42]\n",
      " [ 41  42  43]\n",
      " [ 42  43  44]\n",
      " [ 43  44  45]\n",
      " [ 44  45  46]\n",
      " [ 45  46  47]\n",
      " [ 46  47  48]\n",
      " [ 47  48  49]\n",
      " [ 48  49  50]\n",
      " [ 49  50  51]\n",
      " [ 50  51  52]\n",
      " [ 51  52  53]\n",
      " [ 52  53  54]\n",
      " [ 53  54  55]\n",
      " [ 54  55  56]\n",
      " [ 55  56  57]\n",
      " [ 56  57  58]\n",
      " [ 57  58  59]\n",
      " [ 58  59  60]\n",
      " [ 59  60  61]\n",
      " [ 60  61  62]\n",
      " [ 61  62  63]\n",
      " [ 62  63  64]\n",
      " [ 63  64  65]\n",
      " [ 64  65  66]\n",
      " [ 65  66  67]\n",
      " [ 66  67  68]\n",
      " [ 67  68  69]\n",
      " [ 68  69  70]\n",
      " [ 69  70  71]\n",
      " [ 70  71  72]\n",
      " [ 71  72  73]\n",
      " [ 72  73  74]\n",
      " [ 73  74  75]\n",
      " [ 74  75  76]\n",
      " [ 75  76  77]\n",
      " [ 76  77  78]\n",
      " [ 77  78  79]\n",
      " [ 78  79  80]\n",
      " [ 79  80  81]\n",
      " [ 80  81  82]\n",
      " [ 81  82  83]\n",
      " [ 82  83  84]\n",
      " [ 83  84  85]\n",
      " [ 84  85  86]\n",
      " [ 85  86  87]\n",
      " [ 86  87  88]\n",
      " [ 87  88  89]\n",
      " [ 88  89  90]\n",
      " [ 89  90  91]\n",
      " [ 90  91  92]\n",
      " [ 91  92  93]\n",
      " [ 92  93  94]\n",
      " [ 93  94  95]\n",
      " [ 94  95  96]\n",
      " [ 95  96  97]\n",
      " [ 96  97  98]\n",
      " [ 97  98  99]\n",
      " [ 98  99 100]\n",
      " [ 99 100 101]\n",
      " [100 101 102]\n",
      " [101 102 103]\n",
      " [102 103 104]\n",
      " [103 104 105]\n",
      " [104 105 106]\n",
      " [105 106 107]\n",
      " [106 107 108]\n",
      " [107 108 109]\n",
      " [108 109 110]\n",
      " [109 110 111]\n",
      " [110 111 112]\n",
      " [111 112 113]\n",
      " [112 113 114]\n",
      " [113 114 115]\n",
      " [114 115 116]\n",
      " [115 116 117]]\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,_ = test2_slice(dataset,109,(3,7),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "57/57 - 0s - loss: 113.8941 - mse: 113.8941 - val_loss: 6.0097 - val_mse: 6.0097\n",
      "Epoch 2/200\n",
      "57/57 - 0s - loss: 93.3153 - mse: 93.3153 - val_loss: 6.4164 - val_mse: 6.4164\n",
      "Epoch 3/200\n",
      "57/57 - 0s - loss: 91.7608 - mse: 91.7608 - val_loss: 17.0984 - val_mse: 17.0984\n",
      "Epoch 4/200\n",
      "57/57 - 0s - loss: 100.7894 - mse: 100.7894 - val_loss: 20.4975 - val_mse: 20.4975\n",
      "Epoch 5/200\n",
      "57/57 - 0s - loss: 106.1620 - mse: 106.1620 - val_loss: 41.1740 - val_mse: 41.1740\n",
      "Epoch 6/200\n",
      "57/57 - 0s - loss: 98.5047 - mse: 98.5047 - val_loss: 15.5689 - val_mse: 15.5689\n",
      "Epoch 7/200\n",
      "57/57 - 0s - loss: 73.5054 - mse: 73.5054 - val_loss: 21.8641 - val_mse: 21.8641\n",
      "Epoch 8/200\n",
      "57/57 - 0s - loss: 85.6528 - mse: 85.6528 - val_loss: 5.3627 - val_mse: 5.3627\n",
      "Epoch 9/200\n",
      "57/57 - 0s - loss: 106.8669 - mse: 106.8669 - val_loss: 17.8989 - val_mse: 17.8989\n",
      "Epoch 10/200\n",
      "57/57 - 0s - loss: 52.0647 - mse: 52.0647 - val_loss: 2.9290 - val_mse: 2.9290\n",
      "Epoch 11/200\n",
      "57/57 - 0s - loss: 101.7196 - mse: 101.7196 - val_loss: 35.2481 - val_mse: 35.2481\n",
      "Epoch 12/200\n",
      "57/57 - 0s - loss: 85.7506 - mse: 85.7506 - val_loss: 2.2577 - val_mse: 2.2577\n",
      "Epoch 13/200\n",
      "57/57 - 0s - loss: 88.9199 - mse: 88.9199 - val_loss: 3.6975 - val_mse: 3.6975\n",
      "Epoch 14/200\n",
      "57/57 - 0s - loss: 108.3234 - mse: 108.3234 - val_loss: 54.7522 - val_mse: 54.7522\n",
      "Epoch 15/200\n",
      "57/57 - 0s - loss: 94.0837 - mse: 94.0837 - val_loss: 21.6951 - val_mse: 21.6951\n",
      "Epoch 16/200\n",
      "57/57 - 0s - loss: 87.6429 - mse: 87.6429 - val_loss: 5.1793 - val_mse: 5.1793\n",
      "Epoch 17/200\n",
      "57/57 - 0s - loss: 95.2062 - mse: 95.2062 - val_loss: 8.3753 - val_mse: 8.3753\n",
      "Epoch 18/200\n",
      "57/57 - 0s - loss: 99.8082 - mse: 99.8082 - val_loss: 27.8563 - val_mse: 27.8563\n",
      "Epoch 19/200\n",
      "57/57 - 0s - loss: 65.7135 - mse: 65.7135 - val_loss: 7.2280 - val_mse: 7.2280\n",
      "Epoch 20/200\n",
      "57/57 - 0s - loss: 119.0821 - mse: 119.0821 - val_loss: 12.9837 - val_mse: 12.9837\n",
      "Epoch 21/200\n",
      "57/57 - 0s - loss: 158.5351 - mse: 158.5351 - val_loss: 22.3137 - val_mse: 22.3137\n",
      "Epoch 22/200\n",
      "57/57 - 0s - loss: 87.6166 - mse: 87.6166 - val_loss: 6.7925 - val_mse: 6.7925\n",
      "Epoch 23/200\n",
      "57/57 - 0s - loss: 72.0087 - mse: 72.0087 - val_loss: 49.6490 - val_mse: 49.6490\n",
      "Epoch 24/200\n",
      "57/57 - 0s - loss: 47.6985 - mse: 47.6985 - val_loss: 20.6029 - val_mse: 20.6029\n",
      "Epoch 25/200\n",
      "57/57 - 0s - loss: 98.8409 - mse: 98.8409 - val_loss: 26.5152 - val_mse: 26.5152\n",
      "Epoch 26/200\n",
      "57/57 - 0s - loss: 138.7718 - mse: 138.7718 - val_loss: 40.4692 - val_mse: 40.4692\n",
      "Epoch 27/200\n",
      "57/57 - 0s - loss: 105.5258 - mse: 105.5258 - val_loss: 88.9308 - val_mse: 88.9308\n",
      "Epoch 28/200\n",
      "57/57 - 0s - loss: 115.7843 - mse: 115.7843 - val_loss: 14.0194 - val_mse: 14.0194\n",
      "Epoch 29/200\n",
      "57/57 - 0s - loss: 82.3980 - mse: 82.3980 - val_loss: 35.0698 - val_mse: 35.0698\n",
      "Epoch 30/200\n",
      "57/57 - 0s - loss: 68.4071 - mse: 68.4071 - val_loss: 14.3471 - val_mse: 14.3471\n",
      "Epoch 31/200\n",
      "57/57 - 0s - loss: 88.1430 - mse: 88.1430 - val_loss: 6.2939 - val_mse: 6.2939\n",
      "Epoch 32/200\n",
      "57/57 - 0s - loss: 84.2050 - mse: 84.2050 - val_loss: 1.8763 - val_mse: 1.8763\n",
      "Epoch 33/200\n",
      "57/57 - 0s - loss: 74.8284 - mse: 74.8284 - val_loss: 17.7013 - val_mse: 17.7013\n",
      "Epoch 34/200\n",
      "57/57 - 0s - loss: 106.6881 - mse: 106.6881 - val_loss: 25.3402 - val_mse: 25.3402\n",
      "Epoch 35/200\n",
      "57/57 - 0s - loss: 79.2776 - mse: 79.2776 - val_loss: 22.5595 - val_mse: 22.5595\n",
      "Epoch 36/200\n",
      "57/57 - 0s - loss: 84.2686 - mse: 84.2686 - val_loss: 15.7582 - val_mse: 15.7582\n",
      "Epoch 37/200\n",
      "57/57 - 0s - loss: 64.7046 - mse: 64.7046 - val_loss: 135.2009 - val_mse: 135.2009\n",
      "Epoch 38/200\n",
      "57/57 - 0s - loss: 83.6308 - mse: 83.6308 - val_loss: 3.4507 - val_mse: 3.4507\n",
      "Epoch 39/200\n",
      "57/57 - 0s - loss: 75.9408 - mse: 75.9408 - val_loss: 14.1713 - val_mse: 14.1713\n",
      "Epoch 40/200\n",
      "57/57 - 0s - loss: 86.5625 - mse: 86.5625 - val_loss: 27.8544 - val_mse: 27.8544\n",
      "Epoch 41/200\n",
      "57/57 - 0s - loss: 137.3509 - mse: 137.3509 - val_loss: 44.1027 - val_mse: 44.1027\n",
      "Epoch 42/200\n",
      "57/57 - 0s - loss: 74.6589 - mse: 74.6589 - val_loss: 35.9789 - val_mse: 35.9789\n",
      "Epoch 43/200\n",
      "57/57 - 0s - loss: 114.3023 - mse: 114.3023 - val_loss: 11.3291 - val_mse: 11.3291\n",
      "Epoch 44/200\n",
      "57/57 - 0s - loss: 79.6528 - mse: 79.6528 - val_loss: 3.7358 - val_mse: 3.7358\n",
      "Epoch 45/200\n",
      "57/57 - 0s - loss: 50.2377 - mse: 50.2377 - val_loss: 16.0236 - val_mse: 16.0236\n",
      "Epoch 46/200\n",
      "57/57 - 0s - loss: 125.4279 - mse: 125.4279 - val_loss: 42.1445 - val_mse: 42.1445\n",
      "Epoch 47/200\n",
      "57/57 - 0s - loss: 86.0056 - mse: 86.0056 - val_loss: 13.7204 - val_mse: 13.7204\n",
      "Epoch 48/200\n",
      "57/57 - 0s - loss: 57.3741 - mse: 57.3741 - val_loss: 23.2211 - val_mse: 23.2211\n",
      "Epoch 49/200\n",
      "57/57 - 0s - loss: 70.5149 - mse: 70.5149 - val_loss: 21.2845 - val_mse: 21.2845\n",
      "Epoch 50/200\n",
      "57/57 - 0s - loss: 69.6580 - mse: 69.6580 - val_loss: 33.7598 - val_mse: 33.7598\n",
      "Epoch 51/200\n",
      "57/57 - 0s - loss: 48.1140 - mse: 48.1140 - val_loss: 1.6433 - val_mse: 1.6433\n",
      "Epoch 52/200\n",
      "57/57 - 0s - loss: 85.8754 - mse: 85.8754 - val_loss: 44.9768 - val_mse: 44.9768\n",
      "Epoch 53/200\n",
      "57/57 - 0s - loss: 151.6131 - mse: 151.6131 - val_loss: 6.3511 - val_mse: 6.3511\n",
      "Epoch 54/200\n",
      "57/57 - 0s - loss: 90.0663 - mse: 90.0663 - val_loss: 35.1513 - val_mse: 35.1513\n",
      "Epoch 55/200\n",
      "57/57 - 0s - loss: 111.1041 - mse: 111.1041 - val_loss: 109.0644 - val_mse: 109.0644\n",
      "Epoch 56/200\n",
      "57/57 - 0s - loss: 77.4092 - mse: 77.4092 - val_loss: 32.6395 - val_mse: 32.6395\n",
      "Epoch 57/200\n",
      "57/57 - 0s - loss: 82.5368 - mse: 82.5368 - val_loss: 65.0635 - val_mse: 65.0635\n",
      "Epoch 58/200\n",
      "57/57 - 0s - loss: 89.7483 - mse: 89.7483 - val_loss: 9.7496 - val_mse: 9.7496\n",
      "Epoch 59/200\n",
      "57/57 - 0s - loss: 78.9273 - mse: 78.9273 - val_loss: 6.4962 - val_mse: 6.4962\n",
      "Epoch 60/200\n",
      "57/57 - 0s - loss: 53.1341 - mse: 53.1341 - val_loss: 6.8581 - val_mse: 6.8581\n",
      "Epoch 61/200\n",
      "57/57 - 0s - loss: 49.5132 - mse: 49.5132 - val_loss: 9.3199 - val_mse: 9.3199\n",
      "Epoch 62/200\n",
      "57/57 - 0s - loss: 68.0084 - mse: 68.0084 - val_loss: 6.9319 - val_mse: 6.9319\n",
      "Epoch 63/200\n",
      "57/57 - 0s - loss: 101.1101 - mse: 101.1101 - val_loss: 18.8395 - val_mse: 18.8395\n",
      "Epoch 64/200\n",
      "57/57 - 0s - loss: 104.6403 - mse: 104.6403 - val_loss: 24.9838 - val_mse: 24.9838\n",
      "Epoch 65/200\n",
      "57/57 - 0s - loss: 82.0062 - mse: 82.0062 - val_loss: 67.2075 - val_mse: 67.2075\n",
      "Epoch 66/200\n",
      "57/57 - 0s - loss: 90.8348 - mse: 90.8348 - val_loss: 6.6894 - val_mse: 6.6894\n",
      "Epoch 67/200\n",
      "57/57 - 0s - loss: 65.7815 - mse: 65.7815 - val_loss: 10.6261 - val_mse: 10.6261\n",
      "Epoch 68/200\n",
      "57/57 - 0s - loss: 80.4653 - mse: 80.4653 - val_loss: 7.0214 - val_mse: 7.0214\n",
      "Epoch 69/200\n",
      "57/57 - 0s - loss: 129.5452 - mse: 129.5452 - val_loss: 28.2339 - val_mse: 28.2339\n",
      "Epoch 70/200\n",
      "57/57 - 0s - loss: 101.0285 - mse: 101.0285 - val_loss: 9.5923 - val_mse: 9.5923\n",
      "Epoch 71/200\n",
      "57/57 - 0s - loss: 72.2174 - mse: 72.2174 - val_loss: 2.2652 - val_mse: 2.2652\n",
      "Epoch 72/200\n",
      "57/57 - 0s - loss: 90.4972 - mse: 90.4972 - val_loss: 120.0776 - val_mse: 120.0776\n",
      "Epoch 73/200\n",
      "57/57 - 0s - loss: 87.1886 - mse: 87.1886 - val_loss: 9.3297 - val_mse: 9.3297\n",
      "Epoch 74/200\n",
      "57/57 - 0s - loss: 46.6012 - mse: 46.6012 - val_loss: 5.2292 - val_mse: 5.2292\n",
      "Epoch 75/200\n",
      "57/57 - 0s - loss: 83.1858 - mse: 83.1858 - val_loss: 11.6388 - val_mse: 11.6388\n",
      "Epoch 76/200\n",
      "57/57 - 0s - loss: 115.5198 - mse: 115.5198 - val_loss: 15.5779 - val_mse: 15.5779\n",
      "Epoch 77/200\n",
      "57/57 - 0s - loss: 70.1284 - mse: 70.1284 - val_loss: 4.6000 - val_mse: 4.6000\n",
      "Epoch 78/200\n",
      "57/57 - 0s - loss: 79.8900 - mse: 79.8900 - val_loss: 21.1587 - val_mse: 21.1587\n",
      "Epoch 79/200\n",
      "57/57 - 0s - loss: 84.2993 - mse: 84.2993 - val_loss: 5.1893 - val_mse: 5.1893\n",
      "Epoch 80/200\n",
      "57/57 - 0s - loss: 75.9642 - mse: 75.9642 - val_loss: 21.0108 - val_mse: 21.0108\n",
      "Epoch 81/200\n",
      "57/57 - 0s - loss: 97.9249 - mse: 97.9249 - val_loss: 7.3885 - val_mse: 7.3885\n",
      "Epoch 82/200\n",
      "57/57 - 0s - loss: 73.3311 - mse: 73.3311 - val_loss: 11.4571 - val_mse: 11.4571\n",
      "Epoch 83/200\n",
      "57/57 - 0s - loss: 64.3031 - mse: 64.3031 - val_loss: 4.7267 - val_mse: 4.7267\n",
      "Epoch 84/200\n",
      "57/57 - 0s - loss: 93.2029 - mse: 93.2029 - val_loss: 8.5669 - val_mse: 8.5669\n",
      "Epoch 85/200\n",
      "57/57 - 0s - loss: 76.1040 - mse: 76.1040 - val_loss: 19.6919 - val_mse: 19.6919\n",
      "Epoch 86/200\n",
      "57/57 - 0s - loss: 89.6872 - mse: 89.6872 - val_loss: 38.6599 - val_mse: 38.6599\n",
      "Epoch 87/200\n",
      "57/57 - 0s - loss: 72.5092 - mse: 72.5092 - val_loss: 46.1193 - val_mse: 46.1193\n",
      "Epoch 88/200\n",
      "57/57 - 0s - loss: 82.6315 - mse: 82.6315 - val_loss: 35.1218 - val_mse: 35.1218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/200\n",
      "57/57 - 0s - loss: 104.7727 - mse: 104.7727 - val_loss: 17.8332 - val_mse: 17.8332\n",
      "Epoch 90/200\n",
      "57/57 - 0s - loss: 102.3087 - mse: 102.3087 - val_loss: 24.7988 - val_mse: 24.7988\n",
      "Epoch 91/200\n",
      "57/57 - 0s - loss: 47.5457 - mse: 47.5457 - val_loss: 11.0866 - val_mse: 11.0866\n",
      "Epoch 92/200\n",
      "57/57 - 0s - loss: 80.2010 - mse: 80.2010 - val_loss: 8.4401 - val_mse: 8.4401\n",
      "Epoch 93/200\n",
      "57/57 - 0s - loss: 40.3756 - mse: 40.3756 - val_loss: 1.8556 - val_mse: 1.8556\n",
      "Epoch 94/200\n",
      "57/57 - 0s - loss: 64.5522 - mse: 64.5522 - val_loss: 5.1625 - val_mse: 5.1625\n",
      "Epoch 95/200\n",
      "57/57 - 0s - loss: 68.4176 - mse: 68.4176 - val_loss: 4.8107 - val_mse: 4.8107\n",
      "Epoch 96/200\n",
      "57/57 - 0s - loss: 109.9704 - mse: 109.9704 - val_loss: 64.0521 - val_mse: 64.0521\n",
      "Epoch 97/200\n",
      "57/57 - 0s - loss: 109.1821 - mse: 109.1821 - val_loss: 51.6407 - val_mse: 51.6407\n",
      "Epoch 98/200\n",
      "57/57 - 0s - loss: 65.9830 - mse: 65.9830 - val_loss: 24.3615 - val_mse: 24.3615\n",
      "Epoch 99/200\n",
      "57/57 - 0s - loss: 65.9851 - mse: 65.9851 - val_loss: 47.0633 - val_mse: 47.0633\n",
      "Epoch 100/200\n",
      "57/57 - 0s - loss: 50.8034 - mse: 50.8034 - val_loss: 12.0565 - val_mse: 12.0565\n",
      "Epoch 101/200\n",
      "57/57 - 0s - loss: 68.2921 - mse: 68.2921 - val_loss: 2.4226 - val_mse: 2.4226\n",
      "Epoch 00101: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e29f7937c8>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "mc = ModelCheckpoint('./best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss = 'mse', metrics = ['mse'])\n",
    "model.fit(x_train, y_train, epochs=200,  \n",
    "\n",
    "                 batch_size=1, verbose=2, validation_split=0.2,   \n",
    "\n",
    "                 callbacks=[es, mc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3895 - mse: 3.3895\n",
      "test loss :    3.3895139694213867    mse   :  3.3895139694213867\n",
      "y_predict :   [[97.58179 98.04633 99.33859]]\n"
     ]
    }
   ],
   "source": [
    "loss, mse = model.evaluate(x_test,y_test)\n",
    "\n",
    "x_hat = np.array([[[111+i] for i in range(0,7)]])\n",
    "\n",
    "y_predict = model.predict(x_hat)\n",
    "\n",
    "\n",
    "print(f'test loss :    {loss}    mse   :  {mse}')\n",
    "print(f'y_predict :   {y_predict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 30)                3840      \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 64)                1984      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 8,003\n",
      "Trainable params: 8,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "57/57 - 0s - loss: 3538.3730 - val_loss: 3684.3970 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "57/57 - 0s - loss: 2797.2490 - val_loss: 1977.4948 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "57/57 - 0s - loss: 1143.8330 - val_loss: 436.8799 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "57/57 - 0s - loss: 371.7599 - val_loss: 137.1753 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "57/57 - 0s - loss: 178.1578 - val_loss: 86.2159 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "57/57 - 0s - loss: 185.3970 - val_loss: 42.8070 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "57/57 - 0s - loss: 198.2301 - val_loss: 37.3857 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "57/57 - 0s - loss: 139.7647 - val_loss: 18.5413 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "57/57 - 0s - loss: 225.0895 - val_loss: 121.8000 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "57/57 - 0s - loss: 192.0175 - val_loss: 46.5971 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "57/57 - 0s - loss: 331.6205 - val_loss: 46.3846 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "57/57 - 0s - loss: 147.7757 - val_loss: 26.1075 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "57/57 - 0s - loss: 117.1894 - val_loss: 28.9751 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "57/57 - 0s - loss: 196.0174 - val_loss: 24.0438 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "57/57 - 0s - loss: 156.9937 - val_loss: 23.5620 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "57/57 - 0s - loss: 191.2262 - val_loss: 27.8884 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "57/57 - 0s - loss: 186.1621 - val_loss: 163.7419 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "57/57 - 0s - loss: 152.6628 - val_loss: 6.7994 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "57/57 - 0s - loss: 181.3861 - val_loss: 8.8551 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "57/57 - 0s - loss: 170.9121 - val_loss: 54.5402 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "57/57 - 0s - loss: 148.4955 - val_loss: 37.5506 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "57/57 - 0s - loss: 180.0941 - val_loss: 39.8063 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "57/57 - 0s - loss: 215.8070 - val_loss: 19.8567 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "57/57 - 0s - loss: 121.4946 - val_loss: 66.2412 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "57/57 - 0s - loss: 143.0303 - val_loss: 19.4912 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "57/57 - 0s - loss: 228.5479 - val_loss: 7.8361 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "57/57 - 0s - loss: 142.1163 - val_loss: 15.2756 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "57/57 - 0s - loss: 164.4836 - val_loss: 25.3977 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "57/57 - 0s - loss: 205.0611 - val_loss: 5.3015 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "57/57 - 0s - loss: 192.6811 - val_loss: 133.2700 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "57/57 - 0s - loss: 133.5443 - val_loss: 21.9932 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "57/57 - 0s - loss: 118.7515 - val_loss: 60.3294 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "57/57 - 0s - loss: 139.2087 - val_loss: 25.5942 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "57/57 - 0s - loss: 194.6649 - val_loss: 21.7453 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "57/57 - 0s - loss: 122.9529 - val_loss: 14.7611 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "57/57 - 0s - loss: 118.3081 - val_loss: 52.4925 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "57/57 - 0s - loss: 119.7710 - val_loss: 125.0101 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "57/57 - 0s - loss: 224.6464 - val_loss: 31.7419 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "57/57 - 0s - loss: 139.7041 - val_loss: 23.4612 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "57/57 - 0s - loss: 157.1813 - val_loss: 6.6662 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "57/57 - 0s - loss: 145.7104 - val_loss: 15.4277 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "57/57 - 0s - loss: 136.7525 - val_loss: 45.4211 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "57/57 - 0s - loss: 128.2340 - val_loss: 14.8385 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "57/57 - 0s - loss: 146.7848 - val_loss: 30.8742 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "57/57 - 0s - loss: 160.1858 - val_loss: 39.3625 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "57/57 - 0s - loss: 147.4586 - val_loss: 53.3180 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "57/57 - 0s - loss: 168.9364 - val_loss: 21.4741 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "57/57 - 0s - loss: 116.0938 - val_loss: 14.9257 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "57/57 - 0s - loss: 153.1398 - val_loss: 16.1667 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "57/57 - 0s - loss: 133.3628 - val_loss: 9.8634 - lr: 2.0000e-04\n",
      "Epoch 51/500\n",
      "57/57 - 0s - loss: 158.3996 - val_loss: 13.6887 - lr: 2.0000e-04\n",
      "Epoch 52/500\n",
      "57/57 - 0s - loss: 135.5822 - val_loss: 6.8546 - lr: 2.0000e-04\n",
      "Epoch 53/500\n",
      "57/57 - 0s - loss: 99.5723 - val_loss: 33.4103 - lr: 2.0000e-04\n",
      "Epoch 54/500\n",
      "57/57 - 0s - loss: 89.8732 - val_loss: 5.7765 - lr: 2.0000e-04\n",
      "Epoch 55/500\n",
      "57/57 - 0s - loss: 124.8257 - val_loss: 10.5759 - lr: 2.0000e-04\n",
      "Epoch 56/500\n",
      "57/57 - 0s - loss: 130.9356 - val_loss: 5.3302 - lr: 2.0000e-04\n",
      "Epoch 57/500\n",
      "57/57 - 0s - loss: 101.2238 - val_loss: 1.8024 - lr: 2.0000e-04\n",
      "Epoch 58/500\n",
      "57/57 - 0s - loss: 125.3269 - val_loss: 1.1844 - lr: 2.0000e-04\n",
      "Epoch 59/500\n",
      "57/57 - 0s - loss: 165.4851 - val_loss: 26.2326 - lr: 2.0000e-04\n",
      "Epoch 60/500\n",
      "57/57 - 0s - loss: 94.0799 - val_loss: 1.3432 - lr: 2.0000e-04\n",
      "Epoch 61/500\n",
      "57/57 - 0s - loss: 130.6822 - val_loss: 6.0015 - lr: 2.0000e-04\n",
      "Epoch 62/500\n",
      "57/57 - 0s - loss: 93.1158 - val_loss: 9.7670 - lr: 2.0000e-04\n",
      "Epoch 63/500\n",
      "57/57 - 0s - loss: 104.4370 - val_loss: 3.5818 - lr: 2.0000e-04\n",
      "Epoch 64/500\n",
      "57/57 - 0s - loss: 108.4212 - val_loss: 10.9426 - lr: 2.0000e-04\n",
      "Epoch 65/500\n",
      "57/57 - 0s - loss: 138.3549 - val_loss: 12.7985 - lr: 2.0000e-04\n",
      "Epoch 66/500\n",
      "57/57 - 0s - loss: 135.3579 - val_loss: 5.9655 - lr: 2.0000e-04\n",
      "Epoch 67/500\n",
      "57/57 - 0s - loss: 67.2413 - val_loss: 2.5863 - lr: 2.0000e-04\n",
      "Epoch 68/500\n",
      "57/57 - 0s - loss: 106.9959 - val_loss: 1.2479 - lr: 2.0000e-04\n",
      "Epoch 69/500\n",
      "57/57 - 0s - loss: 113.5317 - val_loss: 50.1690 - lr: 2.0000e-04\n",
      "Epoch 70/500\n",
      "57/57 - 0s - loss: 112.2385 - val_loss: 15.8695 - lr: 2.0000e-04\n",
      "Epoch 71/500\n",
      "57/57 - 0s - loss: 110.4871 - val_loss: 1.8367 - lr: 2.0000e-04\n",
      "Epoch 72/500\n",
      "57/57 - 0s - loss: 116.7561 - val_loss: 4.9934 - lr: 2.0000e-04\n",
      "Epoch 73/500\n",
      "57/57 - 0s - loss: 119.3558 - val_loss: 4.9669 - lr: 2.0000e-04\n",
      "Epoch 74/500\n",
      "57/57 - 0s - loss: 85.9734 - val_loss: 1.6420 - lr: 2.0000e-04\n",
      "Epoch 75/500\n",
      "57/57 - 0s - loss: 167.8691 - val_loss: 16.8320 - lr: 2.0000e-04\n",
      "Epoch 76/500\n",
      "57/57 - 0s - loss: 144.4858 - val_loss: 2.0370 - lr: 2.0000e-04\n",
      "Epoch 77/500\n",
      "57/57 - 0s - loss: 136.5794 - val_loss: 8.3292 - lr: 2.0000e-04\n",
      "Epoch 78/500\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "57/57 - 0s - loss: 86.1470 - val_loss: 3.4651 - lr: 2.0000e-04\n",
      "Epoch 79/500\n",
      "57/57 - 0s - loss: 97.7296 - val_loss: 3.7399 - lr: 4.0000e-05\n",
      "Epoch 80/500\n",
      "57/57 - 0s - loss: 112.4100 - val_loss: 2.8893 - lr: 4.0000e-05\n",
      "Epoch 81/500\n",
      "57/57 - 0s - loss: 103.3309 - val_loss: 3.6036 - lr: 4.0000e-05\n",
      "Epoch 82/500\n",
      "57/57 - 0s - loss: 116.4893 - val_loss: 7.9239 - lr: 4.0000e-05\n",
      "Epoch 83/500\n",
      "57/57 - 0s - loss: 85.3374 - val_loss: 5.8837 - lr: 4.0000e-05\n",
      "Epoch 84/500\n",
      "57/57 - 0s - loss: 114.4678 - val_loss: 6.4897 - lr: 4.0000e-05\n",
      "Epoch 85/500\n",
      "57/57 - 0s - loss: 128.2596 - val_loss: 6.7125 - lr: 4.0000e-05\n",
      "Epoch 86/500\n",
      "57/57 - 0s - loss: 84.5895 - val_loss: 6.0083 - lr: 4.0000e-05\n",
      "Epoch 87/500\n",
      "57/57 - 0s - loss: 94.6142 - val_loss: 7.1246 - lr: 4.0000e-05\n",
      "Epoch 88/500\n",
      "57/57 - 0s - loss: 114.8734 - val_loss: 8.9833 - lr: 4.0000e-05\n",
      "Epoch 89/500\n",
      "57/57 - 0s - loss: 111.2721 - val_loss: 9.7554 - lr: 4.0000e-05\n",
      "Epoch 90/500\n",
      "57/57 - 0s - loss: 118.1293 - val_loss: 8.6728 - lr: 4.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/500\n",
      "57/57 - 0s - loss: 85.6787 - val_loss: 5.2103 - lr: 4.0000e-05\n",
      "Epoch 92/500\n",
      "57/57 - 0s - loss: 92.5114 - val_loss: 6.9857 - lr: 4.0000e-05\n",
      "Epoch 93/500\n",
      "57/57 - 0s - loss: 124.3060 - val_loss: 1.9566 - lr: 4.0000e-05\n",
      "Epoch 94/500\n",
      "57/57 - 0s - loss: 94.7114 - val_loss: 1.2500 - lr: 4.0000e-05\n",
      "Epoch 95/500\n",
      "57/57 - 0s - loss: 93.3966 - val_loss: 2.0566 - lr: 4.0000e-05\n",
      "Epoch 96/500\n",
      "57/57 - 0s - loss: 91.9939 - val_loss: 2.5354 - lr: 4.0000e-05\n",
      "Epoch 97/500\n",
      "57/57 - 0s - loss: 104.0391 - val_loss: 3.0205 - lr: 4.0000e-05\n",
      "Epoch 98/500\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "57/57 - 0s - loss: 83.9463 - val_loss: 2.0982 - lr: 4.0000e-05\n",
      "Epoch 99/500\n",
      "57/57 - 0s - loss: 153.7592 - val_loss: 3.0680 - lr: 8.0000e-06\n",
      "Epoch 100/500\n",
      "57/57 - 0s - loss: 113.9750 - val_loss: 3.6836 - lr: 8.0000e-06\n",
      "Epoch 101/500\n",
      "57/57 - 0s - loss: 76.3794 - val_loss: 3.8309 - lr: 8.0000e-06\n",
      "Epoch 102/500\n",
      "57/57 - 0s - loss: 83.6072 - val_loss: 3.8831 - lr: 8.0000e-06\n",
      "Epoch 103/500\n",
      "57/57 - 0s - loss: 48.7007 - val_loss: 3.6663 - lr: 8.0000e-06\n",
      "Epoch 104/500\n",
      "57/57 - 0s - loss: 93.0988 - val_loss: 3.9269 - lr: 8.0000e-06\n",
      "Epoch 105/500\n",
      "57/57 - 0s - loss: 93.6533 - val_loss: 4.9091 - lr: 8.0000e-06\n",
      "Epoch 106/500\n",
      "57/57 - 0s - loss: 147.2020 - val_loss: 6.4384 - lr: 8.0000e-06\n",
      "Epoch 107/500\n",
      "57/57 - 0s - loss: 143.3736 - val_loss: 5.6936 - lr: 8.0000e-06\n",
      "Epoch 108/500\n",
      "57/57 - 0s - loss: 131.7097 - val_loss: 4.3746 - lr: 8.0000e-06\n",
      "Epoch 00108: early stopping\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.8095\n",
      "test loss :    4.809536457061768\n",
      "y_predict :   [[102.47807  104.081024 104.46595 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, LSTM, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "\n",
    "\n",
    "dataset = np.array(range(1,101))\n",
    "\n",
    "def test_1_slice(dataset,x,y):\n",
    "    data = []\n",
    "    for i in range(len(dataset)-x-y+1):\n",
    "        data.append(dataset[i:i+x+y])\n",
    "    data = np.array(data)\n",
    "    \n",
    "    x_data = data[:,0:x].reshape(-1,7,1)\n",
    "    y_data = data[:,x:x+y]\n",
    "    \n",
    "    return x_data,y_data\n",
    "\n",
    "x,y = test_1_slice(dataset,7,3)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(30, input_shape = (7,1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3))\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "mc = ModelCheckpoint('./best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=20, verbose=1, mode='min')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss = 'mse')\n",
    "model.fit(x_train,y_train,epochs=500,validation_split=0.2,batch_size=1,\n",
    "            verbose=2,callbacks=[es, mc,reduce_lr])\n",
    "\n",
    "loss = model.evaluate(x_test,y_test)\n",
    "\n",
    "x_hat = np.array([[[111+i] for i in range(0,7)]])\n",
    "\n",
    "y_predict = model.predict(x_hat)\n",
    "\n",
    "\n",
    "print(f'test loss :    {loss}')\n",
    "print(f'y_predict :   {y_predict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 30)                4560      \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 64)                1984      \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 8,723\n",
      "Trainable params: 8,723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "67/67 - 0s - loss: 4492.7422 - val_loss: 4787.3867 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "67/67 - 0s - loss: 2856.7156 - val_loss: 1725.0353 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "67/67 - 0s - loss: 1070.8198 - val_loss: 767.8483 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "67/67 - 0s - loss: 653.1788 - val_loss: 313.0901 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "67/67 - 0s - loss: 597.0970 - val_loss: 169.1901 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "67/67 - 0s - loss: 335.4987 - val_loss: 138.0099 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "67/67 - 0s - loss: 439.5966 - val_loss: 217.4687 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "67/67 - 0s - loss: 371.3165 - val_loss: 70.8411 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "67/67 - 0s - loss: 262.4123 - val_loss: 95.0409 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "67/67 - 0s - loss: 274.1396 - val_loss: 97.9376 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "67/67 - 0s - loss: 417.5277 - val_loss: 136.3386 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "67/67 - 0s - loss: 396.6740 - val_loss: 68.3399 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "67/67 - 0s - loss: 252.3931 - val_loss: 146.7920 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "67/67 - 0s - loss: 337.2776 - val_loss: 125.6753 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "67/67 - 0s - loss: 415.9948 - val_loss: 122.0233 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "67/67 - 0s - loss: 402.4357 - val_loss: 266.0257 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "67/67 - 0s - loss: 424.3636 - val_loss: 61.1671 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "67/67 - 0s - loss: 347.0898 - val_loss: 598.2278 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "67/67 - 0s - loss: 483.7453 - val_loss: 534.5945 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "67/67 - 0s - loss: 485.3304 - val_loss: 121.8934 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "67/67 - 0s - loss: 476.0632 - val_loss: 93.6804 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "67/67 - 0s - loss: 476.4722 - val_loss: 238.0518 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "67/67 - 0s - loss: 455.0687 - val_loss: 117.8940 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "67/67 - 0s - loss: 286.3646 - val_loss: 178.0822 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "67/67 - 0s - loss: 272.2238 - val_loss: 121.0029 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "67/67 - 0s - loss: 342.1297 - val_loss: 34.4481 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "67/67 - 0s - loss: 434.4074 - val_loss: 123.8998 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "67/67 - 0s - loss: 354.1477 - val_loss: 96.6666 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "67/67 - 0s - loss: 310.9590 - val_loss: 94.5507 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "67/67 - 0s - loss: 269.8177 - val_loss: 74.8780 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "67/67 - 0s - loss: 352.6053 - val_loss: 86.0087 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "67/67 - 0s - loss: 343.9629 - val_loss: 90.2099 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "67/67 - 0s - loss: 402.8657 - val_loss: 138.1904 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "67/67 - 0s - loss: 356.2543 - val_loss: 444.6835 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "67/67 - 0s - loss: 266.5023 - val_loss: 81.0403 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "67/67 - 0s - loss: 235.8812 - val_loss: 278.6981 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "67/67 - 0s - loss: 383.1144 - val_loss: 282.0247 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "67/67 - 0s - loss: 277.7455 - val_loss: 107.4248 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "67/67 - 0s - loss: 251.5845 - val_loss: 61.2340 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "67/67 - 0s - loss: 271.3551 - val_loss: 156.8745 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "67/67 - 0s - loss: 472.8473 - val_loss: 459.7636 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "67/67 - 0s - loss: 465.6456 - val_loss: 154.8372 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "67/67 - 0s - loss: 319.8545 - val_loss: 97.3433 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "67/67 - 0s - loss: 329.4607 - val_loss: 99.9153 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "67/67 - 0s - loss: 217.3246 - val_loss: 78.8242 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "67/67 - 0s - loss: 360.4771 - val_loss: 110.0809 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "67/67 - 0s - loss: 213.6149 - val_loss: 66.1228 - lr: 2.0000e-04\n",
      "Epoch 48/500\n",
      "67/67 - 0s - loss: 204.3897 - val_loss: 161.0183 - lr: 2.0000e-04\n",
      "Epoch 49/500\n",
      "67/67 - 0s - loss: 156.0898 - val_loss: 70.1658 - lr: 2.0000e-04\n",
      "Epoch 50/500\n",
      "67/67 - 0s - loss: 193.6636 - val_loss: 69.9267 - lr: 2.0000e-04\n",
      "Epoch 51/500\n",
      "67/67 - 0s - loss: 144.8673 - val_loss: 24.2288 - lr: 2.0000e-04\n",
      "Epoch 52/500\n",
      "67/67 - 0s - loss: 197.4967 - val_loss: 43.0484 - lr: 2.0000e-04\n",
      "Epoch 53/500\n",
      "67/67 - 0s - loss: 188.5726 - val_loss: 15.6924 - lr: 2.0000e-04\n",
      "Epoch 54/500\n",
      "67/67 - 0s - loss: 181.6438 - val_loss: 138.5765 - lr: 2.0000e-04\n",
      "Epoch 55/500\n",
      "67/67 - 0s - loss: 259.0832 - val_loss: 17.2957 - lr: 2.0000e-04\n",
      "Epoch 56/500\n",
      "67/67 - 0s - loss: 143.5997 - val_loss: 70.8262 - lr: 2.0000e-04\n",
      "Epoch 57/500\n",
      "67/67 - 0s - loss: 187.9243 - val_loss: 199.1622 - lr: 2.0000e-04\n",
      "Epoch 58/500\n",
      "67/67 - 0s - loss: 272.4135 - val_loss: 115.7182 - lr: 2.0000e-04\n",
      "Epoch 59/500\n",
      "67/67 - 0s - loss: 156.6453 - val_loss: 122.6889 - lr: 2.0000e-04\n",
      "Epoch 60/500\n",
      "67/67 - 0s - loss: 196.2621 - val_loss: 77.0421 - lr: 2.0000e-04\n",
      "Epoch 61/500\n",
      "67/67 - 0s - loss: 170.7645 - val_loss: 49.2589 - lr: 2.0000e-04\n",
      "Epoch 62/500\n",
      "67/67 - 0s - loss: 191.7686 - val_loss: 72.1727 - lr: 2.0000e-04\n",
      "Epoch 63/500\n",
      "67/67 - 0s - loss: 129.0629 - val_loss: 65.8549 - lr: 2.0000e-04\n",
      "Epoch 64/500\n",
      "67/67 - 0s - loss: 185.9620 - val_loss: 104.2328 - lr: 2.0000e-04\n",
      "Epoch 65/500\n",
      "67/67 - 0s - loss: 201.7815 - val_loss: 7.4797 - lr: 2.0000e-04\n",
      "Epoch 66/500\n",
      "67/67 - 0s - loss: 169.2044 - val_loss: 44.1440 - lr: 2.0000e-04\n",
      "Epoch 67/500\n",
      "67/67 - 0s - loss: 182.0659 - val_loss: 92.6182 - lr: 2.0000e-04\n",
      "Epoch 68/500\n",
      "67/67 - 0s - loss: 134.4535 - val_loss: 65.0235 - lr: 2.0000e-04\n",
      "Epoch 69/500\n",
      "67/67 - 0s - loss: 144.9740 - val_loss: 21.8713 - lr: 2.0000e-04\n",
      "Epoch 70/500\n",
      "67/67 - 0s - loss: 246.9557 - val_loss: 26.4051 - lr: 2.0000e-04\n",
      "Epoch 71/500\n",
      "67/67 - 0s - loss: 146.1248 - val_loss: 108.8067 - lr: 2.0000e-04\n",
      "Epoch 72/500\n",
      "67/67 - 0s - loss: 150.8628 - val_loss: 119.4956 - lr: 2.0000e-04\n",
      "Epoch 73/500\n",
      "67/67 - 0s - loss: 204.3074 - val_loss: 116.9539 - lr: 2.0000e-04\n",
      "Epoch 74/500\n",
      "67/67 - 0s - loss: 185.5099 - val_loss: 62.0241 - lr: 2.0000e-04\n",
      "Epoch 75/500\n",
      "67/67 - 0s - loss: 124.0729 - val_loss: 26.3139 - lr: 2.0000e-04\n",
      "Epoch 76/500\n",
      "67/67 - 0s - loss: 166.0405 - val_loss: 49.4199 - lr: 2.0000e-04\n",
      "Epoch 77/500\n",
      "67/67 - 0s - loss: 135.3813 - val_loss: 34.3119 - lr: 2.0000e-04\n",
      "Epoch 78/500\n",
      "67/67 - 0s - loss: 206.8115 - val_loss: 13.1655 - lr: 2.0000e-04\n",
      "Epoch 79/500\n",
      "67/67 - 0s - loss: 149.9951 - val_loss: 26.8146 - lr: 2.0000e-04\n",
      "Epoch 80/500\n",
      "67/67 - 0s - loss: 143.9429 - val_loss: 54.6218 - lr: 2.0000e-04\n",
      "Epoch 81/500\n",
      "67/67 - 0s - loss: 162.3574 - val_loss: 50.4450 - lr: 2.0000e-04\n",
      "Epoch 82/500\n",
      "67/67 - 0s - loss: 152.7698 - val_loss: 12.5543 - lr: 2.0000e-04\n",
      "Epoch 83/500\n",
      "67/67 - 0s - loss: 196.4039 - val_loss: 63.9789 - lr: 2.0000e-04\n",
      "Epoch 84/500\n",
      "67/67 - 0s - loss: 170.7654 - val_loss: 18.5532 - lr: 2.0000e-04\n",
      "Epoch 85/500\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "67/67 - 0s - loss: 137.3896 - val_loss: 92.9805 - lr: 2.0000e-04\n",
      "Epoch 86/500\n",
      "67/67 - 0s - loss: 148.9359 - val_loss: 27.8452 - lr: 4.0000e-05\n",
      "Epoch 87/500\n",
      "67/67 - 0s - loss: 156.1333 - val_loss: 23.5974 - lr: 4.0000e-05\n",
      "Epoch 88/500\n",
      "67/67 - 0s - loss: 163.0447 - val_loss: 19.3353 - lr: 4.0000e-05\n",
      "Epoch 89/500\n",
      "67/67 - 0s - loss: 168.2353 - val_loss: 25.4824 - lr: 4.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/500\n",
      "67/67 - 0s - loss: 173.8295 - val_loss: 39.1604 - lr: 4.0000e-05\n",
      "Epoch 91/500\n",
      "67/67 - 0s - loss: 165.6468 - val_loss: 26.3117 - lr: 4.0000e-05\n",
      "Epoch 92/500\n",
      "67/67 - 0s - loss: 130.3436 - val_loss: 30.6750 - lr: 4.0000e-05\n",
      "Epoch 93/500\n",
      "67/67 - 0s - loss: 127.9763 - val_loss: 41.9701 - lr: 4.0000e-05\n",
      "Epoch 94/500\n",
      "67/67 - 0s - loss: 141.1998 - val_loss: 39.1333 - lr: 4.0000e-05\n",
      "Epoch 95/500\n",
      "67/67 - 0s - loss: 162.8051 - val_loss: 21.1615 - lr: 4.0000e-05\n",
      "Epoch 96/500\n",
      "67/67 - 0s - loss: 211.4203 - val_loss: 48.8525 - lr: 4.0000e-05\n",
      "Epoch 97/500\n",
      "67/67 - 0s - loss: 146.8564 - val_loss: 41.1188 - lr: 4.0000e-05\n",
      "Epoch 98/500\n",
      "67/67 - 0s - loss: 244.5385 - val_loss: 31.4316 - lr: 4.0000e-05\n",
      "Epoch 99/500\n",
      "67/67 - 0s - loss: 123.5300 - val_loss: 16.6992 - lr: 4.0000e-05\n",
      "Epoch 100/500\n",
      "67/67 - 0s - loss: 134.2956 - val_loss: 55.1583 - lr: 4.0000e-05\n",
      "Epoch 101/500\n",
      "67/67 - 0s - loss: 120.8281 - val_loss: 56.3564 - lr: 4.0000e-05\n",
      "Epoch 102/500\n",
      "67/67 - 0s - loss: 154.6687 - val_loss: 81.8157 - lr: 4.0000e-05\n",
      "Epoch 103/500\n",
      "67/67 - 0s - loss: 151.5374 - val_loss: 27.5173 - lr: 4.0000e-05\n",
      "Epoch 104/500\n",
      "67/67 - 0s - loss: 163.6427 - val_loss: 22.1625 - lr: 4.0000e-05\n",
      "Epoch 105/500\n",
      "\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "67/67 - 0s - loss: 152.7040 - val_loss: 58.1943 - lr: 4.0000e-05\n",
      "Epoch 106/500\n",
      "67/67 - 0s - loss: 160.6779 - val_loss: 40.7916 - lr: 8.0000e-06\n",
      "Epoch 107/500\n",
      "67/67 - 0s - loss: 134.2293 - val_loss: 43.8578 - lr: 8.0000e-06\n",
      "Epoch 108/500\n",
      "67/67 - 0s - loss: 132.1512 - val_loss: 44.1555 - lr: 8.0000e-06\n",
      "Epoch 109/500\n",
      "67/67 - 0s - loss: 147.2832 - val_loss: 35.7926 - lr: 8.0000e-06\n",
      "Epoch 110/500\n",
      "67/67 - 0s - loss: 128.8386 - val_loss: 35.4263 - lr: 8.0000e-06\n",
      "Epoch 111/500\n",
      "67/67 - 0s - loss: 175.3728 - val_loss: 29.2891 - lr: 8.0000e-06\n",
      "Epoch 112/500\n",
      "67/67 - 0s - loss: 146.1348 - val_loss: 25.7393 - lr: 8.0000e-06\n",
      "Epoch 113/500\n",
      "67/67 - 0s - loss: 146.4113 - val_loss: 30.0948 - lr: 8.0000e-06\n",
      "Epoch 114/500\n",
      "67/67 - 0s - loss: 154.9149 - val_loss: 26.8286 - lr: 8.0000e-06\n",
      "Epoch 115/500\n",
      "67/67 - 0s - loss: 114.3103 - val_loss: 24.8805 - lr: 8.0000e-06\n",
      "Epoch 00115: early stopping\n",
      "1/1 [==============================] - 0s 989us/step - loss: 33.8401\n",
      "test loss :    33.84014892578125\n",
      "y_predict :   [[104.535576 105.35499  106.70985 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, LSTM, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "\n",
    "dataset = np.array(range(1,118))\n",
    "\n",
    "def test_2_slice(dataset, x_shape,y):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for i in range(len(dataset)-x_shape[0]-x_shape[1]-y+2):\n",
    "        xlst=[]\n",
    "        ylst=[]\n",
    "        for j in range(x_shape[0]):\n",
    "            xlst.append(dataset[i+j:i+j+x_shape[1]])\n",
    "            ylst.append(dataset[i+j+x_shape[1]+2])\n",
    "        x_data.append(xlst)\n",
    "        y_data.append(ylst)\n",
    "    \n",
    "    return np.array(x_data), np.array(y_data)\n",
    "\n",
    "\n",
    "x,y = test_2_slice(dataset,(3,7),3)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(30, input_shape = (3,7)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3))\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "mc = ModelCheckpoint('./best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=20, verbose=1, mode='min')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss = 'mse')\n",
    "model.fit(x_train,y_train,epochs=500,validation_split=0.2,batch_size=1,\n",
    "            verbose=2,callbacks=[es, mc,reduce_lr])\n",
    "\n",
    "loss = model.evaluate(x_test,y_test)\n",
    "\n",
    "x_hat = np.array([[109+i+j for i in range(0,7)]for j in range(0,3)]).reshape(-1,3,7)\n",
    "\n",
    "y_predict = model.predict(x_hat)\n",
    "\n",
    "\n",
    "print(f'test loss :    {loss}')\n",
    "print(f'y_predict :   {y_predict}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  1,   2,   3, ...,   5,   6,   7],\n",
       "        [  2,   3,   4, ...,   6,   7,   8],\n",
       "        [  3,   4,   5, ...,   7,   8,   9]],\n",
       "\n",
       "       [[  2,   3,   4, ...,   6,   7,   8],\n",
       "        [  3,   4,   5, ...,   7,   8,   9],\n",
       "        [  4,   5,   6, ...,   8,   9,  10]],\n",
       "\n",
       "       [[  3,   4,   5, ...,   7,   8,   9],\n",
       "        [  4,   5,   6, ...,   8,   9,  10],\n",
       "        [  5,   6,   7, ...,   9,  10,  11]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[104, 105, 106, ..., 108, 109, 110],\n",
       "        [105, 106, 107, ..., 109, 110, 111],\n",
       "        [106, 107, 108, ..., 110, 111, 112]],\n",
       "\n",
       "       [[105, 106, 107, ..., 109, 110, 111],\n",
       "        [106, 107, 108, ..., 110, 111, 112],\n",
       "        [107, 108, 109, ..., 111, 112, 113]],\n",
       "\n",
       "       [[106, 107, 108, ..., 110, 111, 112],\n",
       "        [107, 108, 109, ..., 111, 112, 113],\n",
       "        [108, 109, 110, ..., 112, 113, 114]]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[109, 110, 111, 112, 113, 114, 115],\n",
       "       [110, 111, 112, 113, 114, 115, 116],\n",
       "       [111, 112, 113, 114, 115, 116, 117]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = np.array()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
