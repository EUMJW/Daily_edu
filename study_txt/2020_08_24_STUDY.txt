=============================
기울기 소실
보통 activation function에 의해 발생

미분시 0~1의 값으로 나타나 기울기가 작아짐
layer가 점점 쌓일수록 0~1의 값들이 곱해지면서 output과 멀어질수록 기울기가 0으로 수렴하는 현상

sigmoid의 미분계수는 최대치가 0.25 곱해질수록 빠르게 0으로 수렴
tanh의 미분계수는 0~1로 곱해질수록 0으로 수렴 (sigmoid보다는 느리게)
relu의 경우 미분값이 0또는1이라서 기울기소실이 잘 일어나지않음
===============================

기울기 폭주
기울기가 1 이상일 경우 곱해지면서 점점 커져 발산
특히 RNN에서 자주 발생
해결법 batch nomalization, layer normalization, 가중치초기화 (glorot, he)
===============================
L1,L2 Regularization
cost function에 항을 추가하여 모델을 규제 과적합을 방지

L1 Regularization
Lasso
미분계수는 상수, 가중치가 0이될수있음 (중요도가 낮은 특성을 무시해줌)
outlier 영향을 덜받음
미분 불가능한 점이 있어 gradient base learning에 주의

L2 Regularization
ridge
입력특성의 스케일에 민감 standardscaler
미분계수는 가중치에 비례


Dense에 kernel_regularizer로 사용
==============================
batch normalization
mini batch 단위로 mean과 variance를 normalization 하여
covariance shift를 줄여 기울기 소실,폭주를 막아줌
또한 각 피쳐의 scale을 맞춰주어 최적의 loss에 도달하는 시간을 줄여줌

=============================
activation 2개?
model.add(Dense(20, activation = ['relu','sigmoid']))

Could not interpret activation function identifier: ['relu', 'sigmoid']
불가능

model.add(Dense(20, activation = 'relu'))
model.add(Activation('sigmoid'))
이런식으로는 가능
=============================
PCA
주성분분석 차원축소알고리즘
SVD를 통해 분산이 최대로 보존되는 축을 선택(데이터 손실 최소화)하고 해당축과 직교하며 분산이 최대인 축을 선택을 반복
이미지 데이터에 PCA를 적용한다면
이미지가 28*28이라면 차원은 784
이를 PCA를 통해 k개 만큼의 주성분 벡터를 사용하면 k차원으로 축소되며
차원감소, 데이터압축, 노이즈제거가 된다

pca.explained_variance_ratio_  를 통해 각 차원이 설명하는 분산을 구할 수 있으며
PCA(n_components= 0~1) 를 통해 원하는 퍼센트까지 분산을 유지시키는 PCA이 몇차원인지를 구할수있다

IPCA : 데이터셋이 큰 경우 배치사이즈를 조절하여 PCA를 할 수 있다.

===========================