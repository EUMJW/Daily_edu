{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled18.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVNHlKVJcAXK",
        "outputId": "7c194b2a-e0f0-4a68-9aae-76935a90d230",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-de422b27-371c-4501-b9de-6344d04b2af3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-de422b27-371c-4501-b9de-6344d04b2af3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 금리.csv to 금리.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'금리.csv': b'\\xef\\xbb\\xbf2018-01-01,\\r\\n2018-01-02,2.119\\r\\n2018-01-03,2.127\\r\\n2018-01-04,2.142\\r\\n2018-01-05,2.099\\r\\n2018-01-06,\\r\\n2018-01-07,\\r\\n2018-01-08,2.135\\r\\n2018-01-09,2.149\\r\\n2018-01-10,2.172\\r\\n2018-01-11,2.152\\r\\n2018-01-12,2.174\\r\\n2018-01-13,\\r\\n2018-01-14,\\r\\n2018-01-15,2.223\\r\\n2018-01-16,2.225\\r\\n2018-01-17,2.227\\r\\n2018-01-18,2.172\\r\\n2018-01-19,2.17\\r\\n2018-01-20,\\r\\n2018-01-21,\\r\\n2018-01-22,2.203\\r\\n2018-01-23,2.192\\r\\n2018-01-24,2.197\\r\\n2018-01-25,2.215\\r\\n2018-01-26,2.222\\r\\n2018-01-27,\\r\\n2018-01-28,\\r\\n2018-01-29,2.281\\r\\n2018-01-30,2.304\\r\\n2018-01-31,2.274\\r\\n2018-02-01,2.249\\r\\n2018-02-02,2.25\\r\\n2018-02-03,\\r\\n2018-02-04,\\r\\n2018-02-05,2.287\\r\\n2018-02-06,2.253\\r\\n2018-02-07,2.248\\r\\n2018-02-08,2.272\\r\\n2018-02-09,2.278\\r\\n2018-02-10,\\r\\n2018-02-11,\\r\\n2018-02-12,2.302\\r\\n2018-02-13,2.278\\r\\n2018-02-14,2.265\\r\\n2018-02-15,\\r\\n2018-02-16,\\r\\n2018-02-17,\\r\\n2018-02-18,\\r\\n2018-02-19,2.302\\r\\n2018-02-20,2.316\\r\\n2018-02-21,2.306\\r\\n2018-02-22,2.303\\r\\n2018-02-23,2.285\\r\\n2018-02-24,\\r\\n2018-02-25,\\r\\n2018-02-26,2.263\\r\\n2018-02-27,2.258\\r\\n2018-02-28,2.266\\r\\n2018-03-01,\\r\\n2018-03-02,2.29\\r\\n2018-03-03,\\r\\n2018-03-04,\\r\\n2018-03-05,2.311\\r\\n2018-03-06,2.312\\r\\n2018-03-07,2.297\\r\\n2018-03-08,2.286\\r\\n2018-03-09,2.293\\r\\n2018-03-10,\\r\\n2018-03-11,\\r\\n2018-03-12,2.305\\r\\n2018-03-13,2.301\\r\\n2018-03-14,2.274\\r\\n2018-03-15,2.268\\r\\n2018-03-16,2.276\\r\\n2018-03-17,\\r\\n2018-03-18,\\r\\n2018-03-19,2.275\\r\\n2018-03-20,2.286\\r\\n2018-03-21,2.291\\r\\n2018-03-22,2.256\\r\\n2018-03-23,2.223\\r\\n2018-03-24,\\r\\n2018-03-25,\\r\\n2018-03-26,2.244\\r\\n2018-03-27,2.232\\r\\n2018-03-28,2.222\\r\\n2018-03-29,2.227\\r\\n2018-03-30,2.216\\r\\n2018-03-31,\\r\\n2018-04-01,\\r\\n2018-04-02,2.225\\r\\n2018-04-03,2.193\\r\\n2018-04-04,2.168\\r\\n2018-04-05,2.175\\r\\n2018-04-06,2.156\\r\\n2018-04-07,\\r\\n2018-04-08,\\r\\n2018-04-09,2.167\\r\\n2018-04-10,2.185\\r\\n2018-04-11,2.165\\r\\n2018-04-12,2.156\\r\\n2018-04-13,2.155\\r\\n2018-04-14,\\r\\n2018-04-15,\\r\\n2018-04-16,2.169\\r\\n2018-04-17,2.178\\r\\n2018-04-18,2.196\\r\\n2018-04-19,2.195\\r\\n2018-04-20,2.2\\r\\n2018-04-21,\\r\\n2018-04-22,\\r\\n2018-04-23,2.235\\r\\n2018-04-24,2.227\\r\\n2018-04-25,2.246\\r\\n2018-04-26,2.234\\r\\n2018-04-27,2.201\\r\\n2018-04-28,\\r\\n2018-04-29,\\r\\n2018-04-30,2.216\\r\\n2018-05-01,\\r\\n2018-05-02,2.237\\r\\n2018-05-03,2.246\\r\\n2018-05-04,2.28\\r\\n2018-05-05,\\r\\n2018-05-06,\\r\\n2018-05-07,\\r\\n2018-05-08,2.311\\r\\n2018-05-09,2.309\\r\\n2018-05-10,2.276\\r\\n2018-05-11,2.286\\r\\n2018-05-12,\\r\\n2018-05-13,\\r\\n2018-05-14,2.309\\r\\n2018-05-15,2.312\\r\\n2018-05-16,2.285\\r\\n2018-05-17,2.262\\r\\n2018-05-18,2.231\\r\\n2018-05-19,\\r\\n2018-05-20,\\r\\n2018-05-21,2.251\\r\\n2018-05-22,\\r\\n2018-05-23,2.235\\r\\n2018-05-24,2.191\\r\\n2018-05-25,2.198\\r\\n2018-05-26,\\r\\n2018-05-27,\\r\\n2018-05-28,2.208\\r\\n2018-05-29,2.189\\r\\n2018-05-30,2.192\\r\\n2018-05-31,2.195\\r\\n2018-06-01,2.193\\r\\n2018-06-02,\\r\\n2018-06-03,\\r\\n2018-06-04,2.213\\r\\n2018-06-05,2.201\\r\\n2018-06-06,\\r\\n2018-06-07,2.217\\r\\n2018-06-08,2.189\\r\\n2018-06-09,\\r\\n2018-06-10,\\r\\n2018-06-11,2.223\\r\\n2018-06-12,2.223\\r\\n2018-06-13,\\r\\n2018-06-14,2.227\\r\\n2018-06-15,2.186\\r\\n2018-06-16,\\r\\n2018-06-17,\\r\\n2018-06-18,2.178\\r\\n2018-06-19,2.164\\r\\n2018-06-20,2.163\\r\\n2018-06-21,2.149\\r\\n2018-06-22,2.147\\r\\n2018-06-23,\\r\\n2018-06-24,\\r\\n2018-06-25,2.143\\r\\n2018-06-26,2.148\\r\\n2018-06-27,2.122\\r\\n2018-06-28,2.115\\r\\n2018-06-29,2.122\\r\\n2018-06-30,\\r\\n2018-07-01,\\r\\n2018-07-02,2.123\\r\\n2018-07-03,2.138\\r\\n2018-07-04,2.116\\r\\n2018-07-05,2.093\\r\\n2018-07-06,2.101\\r\\n2018-07-07,\\r\\n2018-07-08,\\r\\n2018-07-09,2.107\\r\\n2018-07-10,2.095\\r\\n2018-07-11,2.054\\r\\n2018-07-12,2.09\\r\\n2018-07-13,2.105\\r\\n2018-07-14,\\r\\n2018-07-15,\\r\\n2018-07-16,2.088\\r\\n2018-07-17,2.094\\r\\n2018-07-18,2.087\\r\\n2018-07-19,2.089\\r\\n2018-07-20,2.077\\r\\n2018-07-21,\\r\\n2018-07-22,\\r\\n2018-07-23,2.077\\r\\n2018-07-24,2.092\\r\\n2018-07-25,2.079\\r\\n2018-07-26,2.071\\r\\n2018-07-27,2.112\\r\\n2018-07-28,\\r\\n2018-07-29,\\r\\n2018-07-30,2.128\\r\\n2018-07-31,2.123\\r\\n2018-08-01,2.124\\r\\n2018-08-02,2.113\\r\\n2018-08-03,2.108\\r\\n2018-08-04,\\r\\n2018-08-05,\\r\\n2018-08-06,2.097\\r\\n2018-08-07,2.082\\r\\n2018-08-08,2.07\\r\\n2018-08-09,2.05\\r\\n2018-08-10,2.04\\r\\n2018-08-11,\\r\\n2018-08-12,\\r\\n2018-08-13,2.051\\r\\n2018-08-14,2.063\\r\\n2018-08-15,\\r\\n2018-08-16,2.05\\r\\n2018-08-17,1.997\\r\\n2018-08-18,\\r\\n2018-08-19,\\r\\n2018-08-20,1.985\\r\\n2018-08-21,1.919\\r\\n2018-08-22,1.962\\r\\n2018-08-23,1.963\\r\\n2018-08-24,1.963\\r\\n2018-08-25,\\r\\n2018-08-26,\\r\\n2018-08-27,1.963\\r\\n2018-08-28,1.965\\r\\n2018-08-29,1.955\\r\\n2018-08-30,1.98\\r\\n2018-08-31,1.916\\r\\n2018-09-01,\\r\\n2018-09-02,\\r\\n2018-09-03,1.922\\r\\n2018-09-04,1.917\\r\\n2018-09-05,1.9\\r\\n2018-09-06,1.912\\r\\n2018-09-07,1.919\\r\\n2018-09-08,\\r\\n2018-09-09,\\r\\n2018-09-10,1.918\\r\\n2018-09-11,1.923\\r\\n2018-09-12,1.893\\r\\n2018-09-13,1.921\\r\\n2018-09-14,1.96\\r\\n2018-09-15,\\r\\n2018-09-16,\\r\\n2018-09-17,1.967\\r\\n2018-09-18,1.98\\r\\n2018-09-19,1.996\\r\\n2018-09-20,2.035\\r\\n2018-09-21,2.022\\r\\n2018-09-22,\\r\\n2018-09-23,\\r\\n2018-09-24,\\r\\n2018-09-25,\\r\\n2018-09-26,\\r\\n2018-09-27,2.006\\r\\n2018-09-28,2.005\\r\\n2018-09-29,\\r\\n2018-09-30,\\r\\n2018-10-01,2.022\\r\\n2018-10-02,2.015\\r\\n2018-10-03,\\r\\n2018-10-04,2.066\\r\\n2018-10-05,2.084\\r\\n2018-10-06,\\r\\n2018-10-07,\\r\\n2018-10-08,2.091\\r\\n2018-10-09,\\r\\n2018-10-10,2.059\\r\\n2018-10-11,2.012\\r\\n2018-10-12,2.05\\r\\n2018-10-13,\\r\\n2018-10-14,\\r\\n2018-10-15,2.051\\r\\n2018-10-16,2.048\\r\\n2018-10-17,2.023\\r\\n2018-10-18,1.981\\r\\n2018-10-19,1.989\\r\\n2018-10-20,\\r\\n2018-10-21,\\r\\n2018-10-22,2.009\\r\\n2018-10-23,1.989\\r\\n2018-10-24,2.007\\r\\n2018-10-25,1.982\\r\\n2018-10-26,1.968\\r\\n2018-10-27,\\r\\n2018-10-28,\\r\\n2018-10-29,1.894\\r\\n2018-10-30,1.931\\r\\n2018-10-31,1.939\\r\\n2018-11-01,1.942\\r\\n2018-11-02,1.982\\r\\n2018-11-03,\\r\\n2018-11-04,\\r\\n2018-11-05,1.982\\r\\n2018-11-06,1.984\\r\\n2018-11-07,1.964\\r\\n2018-11-08,1.966\\r\\n2018-11-09,1.96\\r\\n2018-11-10,\\r\\n2018-11-11,\\r\\n2018-11-12,1.945\\r\\n2018-11-13,1.94\\r\\n2018-11-14,1.922\\r\\n2018-11-15,1.943\\r\\n2018-11-16,1.947\\r\\n2018-11-17,\\r\\n2018-11-18,\\r\\n2018-11-19,1.942\\r\\n2018-11-20,1.94\\r\\n2018-11-21,1.946\\r\\n2018-11-22,1.942\\r\\n2018-11-23,1.914\\r\\n2018-11-24,\\r\\n2018-11-25,\\r\\n2018-11-26,1.914\\r\\n2018-11-27,1.908\\r\\n2018-11-28,1.92\\r\\n2018-11-29,1.889\\r\\n2018-11-30,1.897\\r\\n2018-12-01,\\r\\n2018-12-02,\\r\\n2018-12-03,1.924\\r\\n2018-12-04,1.914\\r\\n2018-12-05,1.901\\r\\n2018-12-06,1.839\\r\\n2018-12-07,1.834\\r\\n2018-12-08,\\r\\n2018-12-09,\\r\\n2018-12-10,1.803\\r\\n2018-12-11,1.792\\r\\n2018-12-12,1.803\\r\\n2018-12-13,1.795\\r\\n2018-12-14,1.781\\r\\n2018-12-15,\\r\\n2018-12-16,\\r\\n2018-12-17,1.795\\r\\n2018-12-18,1.795\\r\\n2018-12-19,1.783\\r\\n2018-12-20,1.8\\r\\n2018-12-21,1.838\\r\\n2018-12-22,\\r\\n2018-12-23,\\r\\n2018-12-24,1.846\\r\\n2018-12-25,\\r\\n2018-12-26,1.807\\r\\n2018-12-27,1.81\\r\\n2018-12-28,1.817\\r\\n2018-12-29,\\r\\n2018-12-30,\\r\\n2018-12-31,1.817\\r\\n2019-01-01,\\r\\n2019-01-02,1.802\\r\\n2019-01-03,1.796\\r\\n2019-01-04,1.797\\r\\n2019-01-05,\\r\\n2019-01-06,\\r\\n2019-01-07,1.807\\r\\n2019-01-08,1.81\\r\\n2019-01-09,1.809\\r\\n2019-01-10,1.796\\r\\n2019-01-11,1.804\\r\\n2019-01-12,\\r\\n2019-01-13,\\r\\n2019-01-14,1.797\\r\\n2019-01-15,1.797\\r\\n2019-01-16,1.803\\r\\n2019-01-17,1.795\\r\\n2019-01-18,1.814\\r\\n2019-01-19,\\r\\n2019-01-20,\\r\\n2019-01-21,1.821\\r\\n2019-01-22,1.803\\r\\n2019-01-23,1.803\\r\\n2019-01-24,1.81\\r\\n2019-01-25,1.812\\r\\n2019-01-26,\\r\\n2019-01-27,\\r\\n2019-01-28,1.82\\r\\n2019-01-29,1.823\\r\\n2019-01-30,1.827\\r\\n2019-01-31,1.809\\r\\n2019-02-01,1.803\\r\\n2019-02-02,\\r\\n2019-02-03,\\r\\n2019-02-04,\\r\\n2019-02-05,\\r\\n2019-02-06,\\r\\n2019-02-07,1.801\\r\\n2019-02-08,1.786\\r\\n2019-02-09,\\r\\n2019-02-10,\\r\\n2019-02-11,1.781\\r\\n2019-02-12,1.796\\r\\n2019-02-13,1.801\\r\\n2019-02-14,1.794\\r\\n2019-02-15,1.779\\r\\n2019-02-16,\\r\\n2019-02-17,\\r\\n2019-02-18,1.804\\r\\n2019-02-19,1.8\\r\\n2019-02-20,1.802\\r\\n2019-02-21,1.811\\r\\n2019-02-22,1.814\\r\\n2019-02-23,\\r\\n2019-02-24,\\r\\n2019-02-25,1.819\\r\\n2019-02-26,1.816\\r\\n2019-02-27,1.808\\r\\n2019-02-28,1.813\\r\\n2019-03-01,\\r\\n2019-03-02,\\r\\n2019-03-03,\\r\\n2019-03-04,1.834\\r\\n2019-03-05,1.836\\r\\n2019-03-06,1.828\\r\\n2019-03-07,1.817\\r\\n2019-03-08,1.802\\r\\n2019-03-09,\\r\\n2019-03-10,\\r\\n2019-03-11,1.81\\r\\n2019-03-12,1.81\\r\\n2019-03-13,1.795\\r\\n2019-03-14,1.793\\r\\n2019-03-15,1.803\\r\\n2019-03-16,\\r\\n2019-03-17,\\r\\n2019-03-18,1.812\\r\\n2019-03-19,1.81\\r\\n2019-03-20,1.813\\r\\n2019-03-21,1.793\\r\\n2019-03-22,1.8\\r\\n2019-03-23,\\r\\n2019-03-24,\\r\\n2019-03-25,1.77\\r\\n2019-03-26,1.763\\r\\n2019-03-27,1.722\\r\\n2019-03-28,1.679\\r\\n2019-03-29,1.69\\r\\n2019-03-30,\\r\\n2019-03-31,\\r\\n2019-04-01,1.726\\r\\n2019-04-02,1.706\\r\\n2019-04-03,1.72\\r\\n2019-04-04,1.72\\r\\n2019-04-05,1.735\\r\\n2019-04-06,\\r\\n2019-04-07,\\r\\n2019-04-08,1.723\\r\\n2019-04-09,1.725\\r\\n2019-04-10,1.731\\r\\n2019-04-11,1.732\\r\\n2019-04-12,1.733\\r\\n2019-04-13,\\r\\n2019-04-14,\\r\\n2019-04-15,1.761\\r\\n2019-04-16,1.765\\r\\n2019-04-17,1.778\\r\\n2019-04-18,1.741\\r\\n2019-04-19,1.756\\r\\n2019-04-20,\\r\\n2019-04-21,\\r\\n2019-04-22,1.769\\r\\n2019-04-23,1.768\\r\\n2019-04-24,1.749\\r\\n2019-04-25,1.724\\r\\n2019-04-26,1.72\\r\\n2019-04-27,\\r\\n2019-04-28,\\r\\n2019-04-29,1.712\\r\\n2019-04-30,1.699\\r\\n2019-05-01,\\r\\n2019-05-02,1.732\\r\\n2019-05-03,1.739\\r\\n2019-05-04,\\r\\n2019-05-05,\\r\\n2019-05-06,\\r\\n2019-05-07,1.723\\r\\n2019-05-08,1.717\\r\\n2019-05-09,1.708\\r\\n2019-05-10,1.719\\r\\n2019-05-11,\\r\\n2019-05-12,\\r\\n2019-05-13,1.721\\r\\n2019-05-14,1.711\\r\\n2019-05-15,1.71\\r\\n2019-05-16,1.675\\r\\n2019-05-17,1.67\\r\\n2019-05-18,\\r\\n2019-05-19,\\r\\n2019-05-20,1.681\\r\\n2019-05-21,1.663\\r\\n2019-05-22,1.667\\r\\n2019-05-23,1.648\\r\\n2019-05-24,1.643\\r\\n2019-05-25,\\r\\n2019-05-26,\\r\\n2019-05-27,1.652\\r\\n2019-05-28,1.654\\r\\n2019-05-29,1.619\\r\\n2019-05-30,1.626\\r\\n2019-05-31,1.587\\r\\n2019-06-01,\\r\\n2019-06-02,\\r\\n2019-06-03,1.575\\r\\n2019-06-04,1.57\\r\\n2019-06-05,1.542\\r\\n2019-06-06,\\r\\n2019-06-07,1.537\\r\\n2019-06-08,\\r\\n2019-06-09,\\r\\n2019-06-10,1.533\\r\\n2019-06-11,1.542\\r\\n2019-06-12,1.469\\r\\n2019-06-13,1.48\\r\\n2019-06-14,1.47\\r\\n2019-06-15,\\r\\n2019-06-16,\\r\\n2019-06-17,1.491\\r\\n2019-06-18,1.493\\r\\n2019-06-19,1.484\\r\\n2019-06-20,1.42\\r\\n2019-06-21,1.44\\r\\n2019-06-22,\\r\\n2019-06-23,\\r\\n2019-06-24,1.44\\r\\n2019-06-25,1.478\\r\\n2019-06-26,1.496\\r\\n2019-06-27,1.494\\r\\n2019-06-28,1.472\\r\\n2019-06-29,\\r\\n2019-06-30,\\r\\n2019-07-01,1.479\\r\\n2019-07-02,1.463\\r\\n2019-07-03,1.429\\r\\n2019-07-04,1.416\\r\\n2019-07-05,1.423\\r\\n2019-07-06,\\r\\n2019-07-07,\\r\\n2019-07-08,1.422\\r\\n2019-07-09,1.424\\r\\n2019-07-10,1.438\\r\\n2019-07-11,1.419\\r\\n2019-07-12,1.424\\r\\n2019-07-13,\\r\\n2019-07-14,\\r\\n2019-07-15,1.434\\r\\n2019-07-16,1.431\\r\\n2019-07-17,1.399\\r\\n2019-07-18,1.345\\r\\n2019-07-19,1.327\\r\\n2019-07-20,\\r\\n2019-07-21,\\r\\n2019-07-22,1.333\\r\\n2019-07-23,1.339\\r\\n2019-07-24,1.321\\r\\n2019-07-25,1.302\\r\\n2019-07-26,1.308\\r\\n2019-07-27,\\r\\n2019-07-28,\\r\\n2019-07-29,1.306\\r\\n2019-07-30,1.301\\r\\n2019-07-31,1.292\\r\\n2019-08-01,1.309\\r\\n2019-08-02,1.26\\r\\n2019-08-03,\\r\\n2019-08-04,\\r\\n2019-08-05,1.172\\r\\n2019-08-06,1.163\\r\\n2019-08-07,1.153\\r\\n2019-08-08,1.165\\r\\n2019-08-09,1.186\\r\\n2019-08-10,\\r\\n2019-08-11,\\r\\n2019-08-12,1.182\\r\\n2019-08-13,1.15\\r\\n2019-08-14,1.149\\r\\n2019-08-15,\\r\\n2019-08-16,1.095\\r\\n2019-08-17,\\r\\n2019-08-18,\\r\\n2019-08-19,1.093\\r\\n2019-08-20,1.101\\r\\n2019-08-21,1.156\\r\\n2019-08-22,1.13\\r\\n2019-08-23,1.169\\r\\n2019-08-24,\\r\\n2019-08-25,\\r\\n2019-08-26,1.121\\r\\n2019-08-27,1.171\\r\\n2019-08-28,1.18\\r\\n2019-08-29,1.167\\r\\n2019-08-30,1.168\\r\\n2019-08-31,\\r\\n2019-09-01,\\r\\n2019-09-02,1.233\\r\\n2019-09-03,1.228\\r\\n2019-09-04,1.241\\r\\n2019-09-05,1.259\\r\\n2019-09-06,1.265\\r\\n2019-09-07,\\r\\n2019-09-08,\\r\\n2019-09-09,1.235\\r\\n2019-09-10,1.242\\r\\n2019-09-11,1.258\\r\\n2019-09-12,\\r\\n2019-09-13,\\r\\n2019-09-14,\\r\\n2019-09-15,\\r\\n2019-09-16,1.348\\r\\n2019-09-17,1.315\\r\\n2019-09-18,1.309\\r\\n2019-09-19,1.329\\r\\n2019-09-20,1.332\\r\\n2019-09-21,\\r\\n2019-09-22,\\r\\n2019-09-23,1.331\\r\\n2019-09-24,1.325\\r\\n2019-09-25,1.304\\r\\n2019-09-26,1.301\\r\\n2019-09-27,1.301\\r\\n2019-09-28,\\r\\n2019-09-29,\\r\\n2019-09-30,1.297\\r\\n2019-10-01,1.323\\r\\n2019-10-02,1.303\\r\\n2019-10-03,\\r\\n2019-10-04,1.21\\r\\n2019-10-05,\\r\\n2019-10-06,\\r\\n2019-10-07,1.232\\r\\n2019-10-08,1.264\\r\\n2019-10-09,\\r\\n2019-10-10,1.276\\r\\n2019-10-11,1.281\\r\\n2019-10-12,\\r\\n2019-10-13,\\r\\n2019-10-14,1.281\\r\\n2019-10-15,1.281\\r\\n2019-10-16,1.32\\r\\n2019-10-17,1.375\\r\\n2019-10-18,1.375\\r\\n2019-10-19,\\r\\n2019-10-20,\\r\\n2019-10-21,1.408\\r\\n2019-10-22,1.379\\r\\n2019-10-23,1.388\\r\\n2019-10-24,1.396\\r\\n2019-10-25,1.435\\r\\n2019-10-26,\\r\\n2019-10-27,\\r\\n2019-10-28,1.523\\r\\n2019-10-29,1.499\\r\\n2019-10-30,1.481\\r\\n2019-10-31,1.466\\r\\n2019-11-01,1.467\\r\\n2019-11-02,\\r\\n2019-11-03,\\r\\n2019-11-04,1.55\\r\\n2019-11-05,1.522\\r\\n2019-11-06,1.53\\r\\n2019-11-07,1.541\\r\\n2019-11-08,1.518\\r\\n2019-11-09,\\r\\n2019-11-10,\\r\\n2019-11-11,1.5\\r\\n2019-11-12,1.564\\r\\n2019-11-13,1.51\\r\\n2019-11-14,1.515\\r\\n2019-11-15,1.513\\r\\n2019-11-16,\\r\\n2019-11-17,\\r\\n2019-11-18,1.518\\r\\n2019-11-19,1.485\\r\\n2019-11-20,1.45\\r\\n2019-11-21,1.462\\r\\n2019-11-22,1.456\\r\\n2019-11-23,\\r\\n2019-11-24,\\r\\n2019-11-25,1.475\\r\\n2019-11-26,1.475\\r\\n2019-11-27,1.456\\r\\n2019-11-28,1.43\\r\\n2019-11-29,1.385\\r\\n2019-11-30,\\r\\n2019-12-01,\\r\\n2019-12-02,1.425\\r\\n2019-12-03,1.46\\r\\n2019-12-04,1.406\\r\\n2019-12-05,1.423\\r\\n2019-12-06,1.43\\r\\n2019-12-07,\\r\\n2019-12-08,\\r\\n2019-12-09,1.412\\r\\n2019-12-10,1.39\\r\\n2019-12-11,1.381\\r\\n2019-12-12,1.382\\r\\n2019-12-13,1.402\\r\\n2019-12-14,\\r\\n2019-12-15,\\r\\n2019-12-16,1.369\\r\\n2019-12-17,1.363\\r\\n2019-12-18,1.357\\r\\n2019-12-19,1.383\\r\\n2019-12-20,1.392\\r\\n2019-12-21,\\r\\n2019-12-22,\\r\\n2019-12-23,1.38\\r\\n2019-12-24,1.37\\r\\n2019-12-25,\\r\\n2019-12-26,1.367\\r\\n2019-12-27,1.37\\r\\n2019-12-28,\\r\\n2019-12-29,\\r\\n2019-12-30,1.36\\r\\n2019-12-31,1.36\\r\\n2020-01-01,\\r\\n2020-01-02,1.327\\r\\n2020-01-03,1.27\\r\\n2020-01-04,\\r\\n2020-01-05,\\r\\n2020-01-06,1.277\\r\\n2020-01-07,1.331\\r\\n2020-01-08,1.363\\r\\n2020-01-09,1.416\\r\\n2020-01-10,1.425\\r\\n2020-01-11,\\r\\n2020-01-12,\\r\\n2020-01-13,1.418\\r\\n2020-01-14,1.386\\r\\n2020-01-15,1.391\\r\\n2020-01-16,1.426\\r\\n2020-01-17,1.433\\r\\n2020-01-18,\\r\\n2020-01-19,\\r\\n2020-01-20,1.455\\r\\n2020-01-21,1.395\\r\\n2020-01-22,1.437\\r\\n2020-01-23,1.424\\r\\n2020-01-24,\\r\\n2020-01-25,\\r\\n2020-01-26,\\r\\n2020-01-27,\\r\\n2020-01-28,1.352\\r\\n2020-01-29,1.33\\r\\n2020-01-30,1.301\\r\\n2020-01-31,1.303\\r\\n2020-02-01,\\r\\n2020-02-02,\\r\\n2020-02-03,1.291\\r\\n2020-02-04,1.331\\r\\n2020-02-05,1.307\\r\\n2020-02-06,1.311\\r\\n2020-02-07,1.28\\r\\n2020-02-08,\\r\\n2020-02-09,\\r\\n2020-02-10,1.296\\r\\n2020-02-11,1.299\\r\\n2020-02-12,1.297\\r\\n2020-02-13,1.275\\r\\n2020-02-14,1.33\\r\\n2020-02-15,\\r\\n2020-02-16,\\r\\n2020-02-17,1.32\\r\\n2020-02-18,1.271\\r\\n2020-02-19,1.284\\r\\n2020-02-20,1.234\\r\\n2020-02-21,1.182\\r\\n2020-02-22,\\r\\n2020-02-23,\\r\\n2020-02-24,1.139\\r\\n2020-02-25,1.171\\r\\n2020-02-26,1.135\\r\\n2020-02-27,1.194\\r\\n2020-02-28,1.104\\r\\n2020-02-29,\\r\\n2020-03-01,\\r\\n2020-03-02,1.128\\r\\n2020-03-03,1.11\\r\\n2020-03-04,1.029\\r\\n2020-03-05,1.051\\r\\n2020-03-06,1.078\\r\\n2020-03-07,\\r\\n2020-03-08,\\r\\n2020-03-09,1.038\\r\\n2020-03-10,1.082\\r\\n2020-03-11,1.086\\r\\n2020-03-12,1.062\\r\\n2020-03-13,1.149\\r\\n2020-03-14,\\r\\n2020-03-15,\\r\\n2020-03-16,1.099\\r\\n2020-03-17,1.03\\r\\n2020-03-18,1.05\\r\\n2020-03-19,1.193\\r\\n2020-03-20,1.107\\r\\n2020-03-21,\\r\\n2020-03-22,\\r\\n2020-03-23,1.153\\r\\n2020-03-24,1.127\\r\\n2020-03-25,1.131\\r\\n2020-03-26,1.067\\r\\n2020-03-27,1.06\\r\\n2020-03-28,\\r\\n2020-03-29,\\r\\n2020-03-30,1.098\\r\\n2020-03-31,1.07\\r\\n2020-04-01,1.092\\r\\n2020-04-02,1.059\\r\\n2020-04-03,1.066\\r\\n2020-04-04,\\r\\n2020-04-05,\\r\\n2020-04-06,1.052\\r\\n2020-04-07,1.047\\r\\n2020-04-08,1.024\\r\\n2020-04-09,0.986\\r\\n2020-04-10,0.97\\r\\n2020-04-11,\\r\\n2020-04-12,\\r\\n2020-04-13,0.996\\r\\n2020-04-14,0.996\\r\\n2020-04-15,\\r\\n2020-04-16,0.982\\r\\n2020-04-17,1.006\\r\\n2020-04-18,\\r\\n2020-04-19,\\r\\n2020-04-20,1.013\\r\\n2020-04-21,1.035\\r\\n2020-04-22,1.046\\r\\n2020-04-23,1.036\\r\\n2020-04-24,1.018\\r\\n2020-04-25,\\r\\n2020-04-26,\\r\\n2020-04-27,1.026\\r\\n2020-04-28,1.033\\r\\n2020-04-29,1.006\\r\\n2020-04-30,\\r\\n2020-05-01,\\r\\n2020-05-02,\\r\\n2020-05-03,\\r\\n2020-05-04,0.975\\r\\n2020-05-05,\\r\\n2020-05-06,0.96\\r\\n2020-05-07,0.946\\r\\n2020-05-08,0.914\\r\\n2020-05-09,\\r\\n2020-05-10,\\r\\n2020-05-11,0.926\\r\\n2020-05-12,0.886\\r\\n2020-05-13,0.856\\r\\n2020-05-14,0.869\\r\\n2020-05-15,0.874\\r\\n2020-05-16,\\r\\n2020-05-17,\\r\\n2020-05-18,0.887\\r\\n2020-05-19,0.876\\r\\n2020-05-20,0.868\\r\\n2020-05-21,0.856\\r\\n2020-05-22,0.837\\r\\n2020-05-23,\\r\\n2020-05-24,\\r\\n2020-05-25,0.815\\r\\n2020-05-26,0.839\\r\\n2020-05-27,0.863\\r\\n2020-05-28,0.818\\r\\n2020-05-29,0.826\\r\\n2020-05-30,\\r\\n2020-05-31,\\r\\n2020-06-01,0.831\\r\\n2020-06-02,0.851\\r\\n2020-06-03,0.866\\r\\n2020-06-04,0.886\\r\\n2020-06-05,0.894\\r\\n2020-06-06,\\r\\n2020-06-07,\\r\\n2020-06-08,0.902\\r\\n2020-06-09,0.861\\r\\n2020-06-10,0.84\\r\\n2020-06-11,0.837\\r\\n2020-06-12,0.841\\r\\n2020-06-13,\\r\\n2020-06-14,\\r\\n2020-06-15,0.861\\r\\n2020-06-16,0.86\\r\\n2020-06-17,0.874\\r\\n2020-06-18,0.842\\r\\n2020-06-19,0.844\\r\\n2020-06-20,\\r\\n2020-06-21,\\r\\n2020-06-22,0.846\\r\\n2020-06-23,0.827\\r\\n2020-06-24,0.819\\r\\n2020-06-25,0.817\\r\\n2020-06-26,0.811\\r\\n2020-06-27,\\r\\n2020-06-28,\\r\\n2020-06-29,0.842\\r\\n2020-06-30,0.842\\r\\n2020-07-01,0.847\\r\\n2020-07-02,0.831\\r\\n2020-07-03,0.838\\r\\n2020-07-04,\\r\\n2020-07-05,\\r\\n2020-07-06,0.854\\r\\n2020-07-07,0.843\\r\\n2020-07-08,0.839\\r\\n2020-07-09,0.84\\r\\n2020-07-10,0.849\\r\\n2020-07-11,\\r\\n2020-07-12,\\r\\n2020-07-13,0.86\\r\\n2020-07-14,0.854\\r\\n2020-07-15,0.847\\r\\n2020-07-16,0.829\\r\\n2020-07-17,0.812\\r\\n2020-07-18,\\r\\n2020-07-19,\\r\\n2020-07-20,0.801\\r\\n2020-07-21,0.818\\r\\n2020-07-22,0.81\\r\\n2020-07-23,0.798\\r\\n2020-07-24,0.799\\r\\n2020-07-25,\\r\\n2020-07-26,\\r\\n2020-07-27,0.803\\r\\n2020-07-28,0.813\\r\\n2020-07-29,0.811\\r\\n2020-07-30,0.799\\r\\n2020-07-31,0.796\\r\\n2020-08-01,\\r\\n2020-08-02,\\r\\n2020-08-03,0.799\\r\\n2020-08-04,0.802\\r\\n2020-08-05,0.795\\r\\n2020-08-06,0.807\\r\\n2020-08-07,0.81\\r\\n2020-08-08,\\r\\n2020-08-09,\\r\\n2020-08-10,0.83\\r\\n2020-08-11,0.818\\r\\n2020-08-12,0.827\\r\\n2020-08-13,0.815\\r\\n2020-08-14,0.828\\r\\n2020-08-15,\\r\\n2020-08-16,\\r\\n2020-08-17,\\r\\n2020-08-18,0.81\\r\\n2020-08-19,0.805\\r\\n2020-08-20,0.813\\r\\n2020-08-21,0.854\\r\\n2020-08-22,\\r\\n2020-08-23,\\r\\n2020-08-24,0.824\\r\\n2020-08-25,0.825\\r\\n2020-08-26,0.835\\r\\n2020-08-27,0.852\\r\\n2020-08-28,0.893\\r\\n2020-08-29,\\r\\n2020-08-30,\\r\\n2020-08-31,0.94\\r\\n2020-09-01,0.977\\r\\n2020-09-02,0.923\\r\\n2020-09-03,0.919\\r\\n2020-09-04,0.929\\r\\n2020-09-05,\\r\\n2020-09-06,\\r\\n2020-09-07,0.973\\r\\n2020-09-08,0.949\\r\\n2020-09-09,0.915\\r\\n2020-09-10,0.915\\r\\n2020-09-11,0.926\\r\\n2020-09-12,\\r\\n2020-09-13,\\r\\n2020-09-14,0.919\\r\\n2020-09-15,0.907\\r\\n2020-09-16,0.91\\r\\n2020-09-17,0.914\\r\\n2020-09-18,0.907\\r\\n2020-09-19,\\r\\n2020-09-20,\\r\\n2020-09-21,0.904\\r\\n2020-09-22,0.897\\r\\n2020-09-23,0.883\\r\\n2020-09-24,0.857\\r\\n2020-09-25,0.855\\r\\n2020-09-26,\\r\\n2020-09-27,\\r\\n2020-09-28,0.843\\r\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fzTsgA-cEHB"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVW_ZXMhcFHF"
      },
      "source": [
        "data = pd.read_csv('kospi.csv', header = None,index_col=0)\n",
        "data.columns = ['시가','저가','고가','종가','거래량']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-faqPRJ8cGMn",
        "outputId": "9c6ce212-c08e-4dc6-fe05-07a4e612571c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>시가</th>\n",
              "      <th>저가</th>\n",
              "      <th>고가</th>\n",
              "      <th>종가</th>\n",
              "      <th>거래량</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2018-04-24</th>\n",
              "      <td>320.30</td>\n",
              "      <td>320.30</td>\n",
              "      <td>320.30</td>\n",
              "      <td>320.30</td>\n",
              "      <td>320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-04-25</th>\n",
              "      <td>318.45</td>\n",
              "      <td>318.45</td>\n",
              "      <td>318.45</td>\n",
              "      <td>318.45</td>\n",
              "      <td>240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-04-26</th>\n",
              "      <td>315.95</td>\n",
              "      <td>315.95</td>\n",
              "      <td>315.95</td>\n",
              "      <td>315.95</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-04-27</th>\n",
              "      <td>320.65</td>\n",
              "      <td>320.65</td>\n",
              "      <td>320.65</td>\n",
              "      <td>320.65</td>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-04-30</th>\n",
              "      <td>323.45</td>\n",
              "      <td>323.45</td>\n",
              "      <td>323.45</td>\n",
              "      <td>323.45</td>\n",
              "      <td>160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-23</th>\n",
              "      <td>309.55</td>\n",
              "      <td>310.25</td>\n",
              "      <td>302.55</td>\n",
              "      <td>309.30</td>\n",
              "      <td>416,059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-24</th>\n",
              "      <td>304.90</td>\n",
              "      <td>305.80</td>\n",
              "      <td>301.15</td>\n",
              "      <td>301.45</td>\n",
              "      <td>382,425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-25</th>\n",
              "      <td>303.85</td>\n",
              "      <td>304.40</td>\n",
              "      <td>301.45</td>\n",
              "      <td>303.40</td>\n",
              "      <td>269,556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-28</th>\n",
              "      <td>306.10</td>\n",
              "      <td>308.15</td>\n",
              "      <td>304.50</td>\n",
              "      <td>307.65</td>\n",
              "      <td>189,300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-29</th>\n",
              "      <td>311.35</td>\n",
              "      <td>311.40</td>\n",
              "      <td>309.35</td>\n",
              "      <td>309.85</td>\n",
              "      <td>88,323</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>600 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                시가      저가      고가      종가      거래량\n",
              "0                                                  \n",
              "2018-04-24  320.30  320.30  320.30  320.30      320\n",
              "2018-04-25  318.45  318.45  318.45  318.45      240\n",
              "2018-04-26  315.95  315.95  315.95  315.95        0\n",
              "2018-04-27  320.65  320.65  320.65  320.65      270\n",
              "2018-04-30  323.45  323.45  323.45  323.45      160\n",
              "...            ...     ...     ...     ...      ...\n",
              "2020-09-23  309.55  310.25  302.55  309.30  416,059\n",
              "2020-09-24  304.90  305.80  301.15  301.45  382,425\n",
              "2020-09-25  303.85  304.40  301.45  303.40  269,556\n",
              "2020-09-28  306.10  308.15  304.50  307.65  189,300\n",
              "2020-09-29  311.35  311.40  309.35  309.85   88,323\n",
              "\n",
              "[600 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27cn2NYLeKhy",
        "outputId": "e0f8f21c-56f9-4d4f-9c6a-d9069a63ff9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "dollar_data = pd.read_csv('원달러.csv', header = None,index_col=0)\n",
        "dollar_data.columns = ['달러시가','달러저가','달러고가','달러종가']\n",
        "dollar_data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>달러시가</th>\n",
              "      <th>달러저가</th>\n",
              "      <th>달러고가</th>\n",
              "      <th>달러종가</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-02-02</th>\n",
              "      <td>1,152.00</td>\n",
              "      <td>1,154.40</td>\n",
              "      <td>1,145.40</td>\n",
              "      <td>1,146.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-03</th>\n",
              "      <td>1,144.80</td>\n",
              "      <td>1,149.00</td>\n",
              "      <td>1,144.20</td>\n",
              "      <td>1,147.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-06</th>\n",
              "      <td>1,138.00</td>\n",
              "      <td>1,140.00</td>\n",
              "      <td>1,135.60</td>\n",
              "      <td>1,137.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-07</th>\n",
              "      <td>1,136.00</td>\n",
              "      <td>1,145.60</td>\n",
              "      <td>1,136.00</td>\n",
              "      <td>1,144.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-08</th>\n",
              "      <td>1,145.00</td>\n",
              "      <td>1,148.10</td>\n",
              "      <td>1,142.00</td>\n",
              "      <td>1,147.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-23</th>\n",
              "      <td>1,162.90</td>\n",
              "      <td>1,166.00</td>\n",
              "      <td>1,161.80</td>\n",
              "      <td>1,164.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-24</th>\n",
              "      <td>1,171.00</td>\n",
              "      <td>1,172.90</td>\n",
              "      <td>1,167.40</td>\n",
              "      <td>1,172.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-25</th>\n",
              "      <td>1,169.50</td>\n",
              "      <td>1,173.80</td>\n",
              "      <td>1,168.70</td>\n",
              "      <td>1,172.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-28</th>\n",
              "      <td>1,173.30</td>\n",
              "      <td>1,174.80</td>\n",
              "      <td>1,172.20</td>\n",
              "      <td>1,173.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-29</th>\n",
              "      <td>1,167.50</td>\n",
              "      <td>1,171.20</td>\n",
              "      <td>1,167.50</td>\n",
              "      <td>1,169.30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>899 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                달러시가      달러저가      달러고가      달러종가\n",
              "0                                                 \n",
              "2017-02-02  1,152.00  1,154.40  1,145.40  1,146.80\n",
              "2017-02-03  1,144.80  1,149.00  1,144.20  1,147.60\n",
              "2017-02-06  1,138.00  1,140.00  1,135.60  1,137.90\n",
              "2017-02-07  1,136.00  1,145.60  1,136.00  1,144.30\n",
              "2017-02-08  1,145.00  1,148.10  1,142.00  1,147.20\n",
              "...              ...       ...       ...       ...\n",
              "2020-09-23  1,162.90  1,166.00  1,161.80  1,164.40\n",
              "2020-09-24  1,171.00  1,172.90  1,167.40  1,172.70\n",
              "2020-09-25  1,169.50  1,173.80  1,168.70  1,172.30\n",
              "2020-09-28  1,173.30  1,174.80  1,172.20  1,173.60\n",
              "2020-09-29  1,167.50  1,171.20  1,167.50  1,169.30\n",
              "\n",
              "[899 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7xoH0Gtz1xw",
        "outputId": "8b774ef1-4c74-40ed-8d68-10ead349bddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "wean_data = pd.read_csv('원위안.csv', header=None,index_col=0)\n",
        "wean_data.columns = ['위안시가','위안저가','위안고가','위안종가']\n",
        "wean_data"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>위안시가</th>\n",
              "      <th>위안저가</th>\n",
              "      <th>위안고가</th>\n",
              "      <th>위안종가</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-11-13</th>\n",
              "      <td>6.64</td>\n",
              "      <td>6.65</td>\n",
              "      <td>6.64</td>\n",
              "      <td>6.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-14</th>\n",
              "      <td>6.64</td>\n",
              "      <td>6.64</td>\n",
              "      <td>6.63</td>\n",
              "      <td>6.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-15</th>\n",
              "      <td>6.64</td>\n",
              "      <td>6.64</td>\n",
              "      <td>6.61</td>\n",
              "      <td>6.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-16</th>\n",
              "      <td>6.62</td>\n",
              "      <td>6.64</td>\n",
              "      <td>6.62</td>\n",
              "      <td>6.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-17</th>\n",
              "      <td>6.63</td>\n",
              "      <td>6.64</td>\n",
              "      <td>6.62</td>\n",
              "      <td>6.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-23</th>\n",
              "      <td>6.78</td>\n",
              "      <td>6.81</td>\n",
              "      <td>6.78</td>\n",
              "      <td>6.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-24</th>\n",
              "      <td>6.81</td>\n",
              "      <td>6.83</td>\n",
              "      <td>6.81</td>\n",
              "      <td>6.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-25</th>\n",
              "      <td>6.83</td>\n",
              "      <td>6.83</td>\n",
              "      <td>6.81</td>\n",
              "      <td>6.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-26</th>\n",
              "      <td>6.83</td>\n",
              "      <td>6.83</td>\n",
              "      <td>6.82</td>\n",
              "      <td>6.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-28</th>\n",
              "      <td>6.82</td>\n",
              "      <td>6.83</td>\n",
              "      <td>6.81</td>\n",
              "      <td>6.81</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>899 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            위안시가  위안저가  위안고가  위안종가\n",
              "0                                 \n",
              "2017-11-13  6.64  6.65  6.64  6.64\n",
              "2017-11-14  6.64  6.64  6.63  6.64\n",
              "2017-11-15  6.64  6.64  6.61  6.62\n",
              "2017-11-16  6.62  6.64  6.62  6.63\n",
              "2017-11-17  6.63  6.64  6.62  6.63\n",
              "...          ...   ...   ...   ...\n",
              "2020-09-23  6.78  6.81  6.78  6.81\n",
              "2020-09-24  6.81  6.83  6.81  6.83\n",
              "2020-09-25  6.83  6.83  6.81  6.83\n",
              "2020-09-26  6.83  6.83  6.82  6.82\n",
              "2020-09-28  6.82  6.83  6.81  6.81\n",
              "\n",
              "[899 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNGGeB1chyMO",
        "outputId": "82e0e071-68fe-4efe-feeb-ea11bcb18082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "bond_data = pd.read_csv('금리.csv', header=None, index_col = 0)\n",
        "bond_data.columns = ['일별금리']\n",
        "bond_data"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>일별금리</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2018-01-01</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-02</th>\n",
              "      <td>2.119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-03</th>\n",
              "      <td>2.127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-04</th>\n",
              "      <td>2.142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-05</th>\n",
              "      <td>2.099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-24</th>\n",
              "      <td>0.857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-25</th>\n",
              "      <td>0.855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-26</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-27</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-28</th>\n",
              "      <td>0.843</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1002 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             일별금리\n",
              "0                \n",
              "2018-01-01    NaN\n",
              "2018-01-02  2.119\n",
              "2018-01-03  2.127\n",
              "2018-01-04  2.142\n",
              "2018-01-05  2.099\n",
              "...           ...\n",
              "2020-09-24  0.857\n",
              "2020-09-25  0.855\n",
              "2020-09-26    NaN\n",
              "2020-09-27    NaN\n",
              "2020-09-28  0.843\n",
              "\n",
              "[1002 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar5n30DRjM5m",
        "outputId": "aade856f-a2cf-4f98-9cf5-322d9b67882e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "gold_data = pd.read_csv('금.csv', index_col = 0)\n",
        "gold_data.columns = ['금종가','-','--','---','금거래량']\n",
        "gold_data"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>금종가</th>\n",
              "      <th>-</th>\n",
              "      <th>--</th>\n",
              "      <th>---</th>\n",
              "      <th>금거래량</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>일자</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-09-29</th>\n",
              "      <td>71,030</td>\n",
              "      <td>▲</td>\n",
              "      <td>680</td>\n",
              "      <td>0.97%</td>\n",
              "      <td>59,910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-28</th>\n",
              "      <td>70,350</td>\n",
              "      <td>▼</td>\n",
              "      <td>-190</td>\n",
              "      <td>-0.27%</td>\n",
              "      <td>82,650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-25</th>\n",
              "      <td>70,540</td>\n",
              "      <td>▲</td>\n",
              "      <td>740</td>\n",
              "      <td>1.06%</td>\n",
              "      <td>76,431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-24</th>\n",
              "      <td>69,800</td>\n",
              "      <td>▼</td>\n",
              "      <td>-500</td>\n",
              "      <td>-0.71%</td>\n",
              "      <td>197,661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-23</th>\n",
              "      <td>70,300</td>\n",
              "      <td>▼</td>\n",
              "      <td>-1,340</td>\n",
              "      <td>-1.87%</td>\n",
              "      <td>133,868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-08-31</th>\n",
              "      <td>47,210</td>\n",
              "      <td>▼</td>\n",
              "      <td>-40</td>\n",
              "      <td>-0.08%</td>\n",
              "      <td>29,018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-08-30</th>\n",
              "      <td>47,250</td>\n",
              "      <td>▼</td>\n",
              "      <td>-170</td>\n",
              "      <td>-0.36%</td>\n",
              "      <td>12,486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-08-29</th>\n",
              "      <td>47,420</td>\n",
              "      <td>▲</td>\n",
              "      <td>680</td>\n",
              "      <td>1.45%</td>\n",
              "      <td>69,655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-08-28</th>\n",
              "      <td>46,740</td>\n",
              "      <td>▼</td>\n",
              "      <td>-20</td>\n",
              "      <td>-0.04%</td>\n",
              "      <td>3,825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-08-25</th>\n",
              "      <td>46,760</td>\n",
              "      <td>▲</td>\n",
              "      <td>10</td>\n",
              "      <td>0.02%</td>\n",
              "      <td>10,145</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>760 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               금종가  -      --     ---     금거래량\n",
              "일자                                            \n",
              "2020-09-29  71,030  ▲     680   0.97%   59,910\n",
              "2020-09-28  70,350  ▼    -190  -0.27%   82,650\n",
              "2020-09-25  70,540  ▲     740   1.06%   76,431\n",
              "2020-09-24  69,800  ▼    -500  -0.71%  197,661\n",
              "2020-09-23  70,300  ▼  -1,340  -1.87%  133,868\n",
              "...            ... ..     ...     ...      ...\n",
              "2017-08-31  47,210  ▼     -40  -0.08%   29,018\n",
              "2017-08-30  47,250  ▼    -170  -0.36%   12,486\n",
              "2017-08-29  47,420  ▲     680   1.45%   69,655\n",
              "2017-08-28  46,740  ▼     -20  -0.04%    3,825\n",
              "2017-08-25  46,760  ▲      10   0.02%   10,145\n",
              "\n",
              "[760 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BTb985y0Agh"
      },
      "source": [
        "dataset = pd.concat([data,dollar_data,wean_data, bond_data,gold_data],axis=1, join='inner')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdvsuS3A1vLe"
      },
      "source": [
        "dataset = np.array(dataset.loc[:,['종가','거래량','달러종가','위안종가','일별금리','금종가','금거래량']])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGMy_w1musSg",
        "outputId": "1cac2dec-149a-40b9-c850-c011690f0482",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "dataset"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[320.3, '320', '1,076.80', ..., 2.227, '45,980', '3,867'],\n",
              "       [318.45, '240', '1,080.60', ..., 2.246, '46,070', '5,910'],\n",
              "       [315.95, '0', '1,080.90', ..., 2.234, '46,050', '5,532'],\n",
              "       ...,\n",
              "       [301.45, '382,425', '1,172.70', ..., 0.857, '69,800', '197,661'],\n",
              "       [303.4, '269,556', '1,172.30', ..., 0.855, '70,540', '76,431'],\n",
              "       [307.65, '189,300', '1,173.60', ..., 0.843, '70,350', '82,650']],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t56TsInQuJV8"
      },
      "source": [
        "for i in dataset:\n",
        "  for j in range(7):\n",
        "    try:\n",
        "      i[j]=float(i[j].replace(',',''))\n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xACSolDOvR1c",
        "outputId": "811f308f-08fd-49c5-cb55-1cf14c84f6a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "dataset"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[320.3, 320.0, 1076.8, ..., 2.227, 45980.0, 3867.0],\n",
              "       [318.45, 240.0, 1080.6, ..., 2.246, 46070.0, 5910.0],\n",
              "       [315.95, 0.0, 1080.9, ..., 2.234, 46050.0, 5532.0],\n",
              "       ...,\n",
              "       [301.45, 382425.0, 1172.7, ..., 0.857, 69800.0, 197661.0],\n",
              "       [303.4, 269556.0, 1172.3, ..., 0.855, 70540.0, 76431.0],\n",
              "       [307.65, 189300.0, 1173.6, ..., 0.843, 70350.0, 82650.0]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9R1obK3vcrp"
      },
      "source": [
        "lst=[]\n",
        "for i in range(len(dataset[:,0])-1):\n",
        "  if dataset[i,0] < dataset[i+1,0]:\n",
        "    lst.append(1)\n",
        "  else:\n",
        "    lst.append(0)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddIK9PL087Zr",
        "outputId": "595b74e4-3c6b-48cd-f1b0-8b15d5dbfbb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_dataset = dataset[1:]\n",
        "y_dataset = np.array(lst)\n",
        "\n",
        "print(x_dataset.shape, y_dataset.shape)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(598, 7) (598,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eXdbixYE6-8"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler()\n",
        "x_dataset = sc.fit_transform(x_dataset)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2plYnLGABLPI"
      },
      "source": [
        "def slice_dataset(x,y,predict_date=4,period=20):\n",
        "  x_return = []\n",
        "  y_return = []\n",
        "  x_last = []\n",
        "  for i in range(len(x)-predict_date-period+1):\n",
        "    x_return.append(x[i:i+period])\n",
        "    y_return.append(y[i+period:i+period+predict_date])\n",
        "  for i in range(len(x)-period+1):\n",
        "    x_last.append(x[i:i+period])\n",
        "\n",
        "  return np.array(x_return, dtype='f8'), np.array(y_return, dtype='f8'), np.array(x_last, dtype='f8')"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSfa_HTnBMEI"
      },
      "source": [
        "x_data, y_data, x_last = slice_dataset(x_dataset,y_dataset,1,20)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFrC7Ji7C6UP",
        "outputId": "8641511b-8d3e-41b8-cf6a-6274176f5502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(x_data.shape,y_data.shape,x_last.shape)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(578, 20, 7) (578, 1) (579, 20, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVMjF9iQvkAd"
      },
      "source": [
        "x_train = x_data[:-100]\n",
        "y_train = y_data[:-100]\n",
        "x_test = x_data[-100:]\n",
        "y_test = y_data[-100:]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M29xa3Vh-Pna",
        "outputId": "5d85eb46-da5c-461a-8ec1-2865155e638f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(478, 20, 7) (478, 1) (100, 20, 7) (100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgzHbAyfj2jn"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYXsQZZpt3SK"
      },
      "source": [
        "RF = RandomForestClassifier()"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X58ewlOht_f9"
      },
      "source": [
        "RF = RF.fit(x_train, y_train)"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGNK9yfduMpI",
        "outputId": "5a387e1c-f4a6-4e9b-d5cb-6357633cbe46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "importances = RF.feature_importances_\n",
        "print(importances)\n"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.16896747 0.09827384 0.16532926 0.1101073  0.15482295 0.15196684\n",
            " 0.15053235]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdZSjwGevu3i"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvLeEPh6v58d"
      },
      "source": [
        "std = np.std([tree.feature_importances_ for tree in RF.estimators_],axis=0)"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ouan_zDpwV4W"
      },
      "source": [
        "indices = np.argsort(importances)[::-1]"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivn6lG1NwYdP",
        "outputId": "584b4e2d-515a-4ef7-8dba-4891e7c5c66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(x_data.shape[1]):\n",
        "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature ranking:\n",
            "1. feature 0 (0.168967)\n",
            "2. feature 2 (0.165329)\n",
            "3. feature 4 (0.154823)\n",
            "4. feature 5 (0.151967)\n",
            "5. feature 6 (0.150532)\n",
            "6. feature 3 (0.110107)\n",
            "7. feature 1 (0.098274)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9H_Au3Dwcio",
        "outputId": "40f69095-ff2c-456b-d853-52e2932c85c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plt.figure()\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(range(x_data.shape[1]), importances[indices],\n",
        "        color=\"r\", yerr=std[indices], align=\"center\")\n",
        "plt.xticks(range(x_data.shape[1]), indices)\n",
        "plt.xlim([-1, x_data.shape[1]])\n",
        "plt.show()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtSb3_pMwmv3",
        "outputId": "faf4e61b-ba5f-4c6a-81bd-49035815f47c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "RF.score(x_test,y_test)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.47"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0B1wXNI99Yy"
      },
      "source": [
        "from xgboost.sklearn import XGBClassifier"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXAdVk4z-_Nv",
        "outputId": "238c83fc-c645-464c-ae76-0eae5d7ceb28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "XGB = XGBClassifier(max_depth=8,n_estimators=400)\n",
        "XGB_fit = XGB.fit(x_train, y_train)"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-202-5ddf04951c44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mXGB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mXGB_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             train_dmatrix = DMatrix(X, label=training_labels,\n\u001b[0;32m--> 726\u001b[0;31m                                     missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         self._Booster = train(xgb_options, train_dmatrix, self.get_num_boosting_rounds(),\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_npy2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_dt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_init_from_npy2d\u001b[0;34m(self, mat, missing, nthread)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \"\"\"\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input numpy.ndarray must be 2 dimensional'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0;31m# flatten the array by rows and ensure it is float32.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;31m# we try to avoid data copies if possible (reshape returns a view when possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input numpy.ndarray must be 2 dimensional"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7Yh5ndt_Mgu",
        "outputId": "cd9b9786-5201-4857-9774-29fea5710245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "XGB_fit.feature_importances_"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.14877902, 0.1508202 , 0.15061603, 0.13421689, 0.15040627,\n",
              "       0.14282918, 0.12233244], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8fxA3Op_nQz",
        "outputId": "27d3980d-af23-40b7-80d3-bef659019168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "source": [
        "XGB_fit.score(x_test,y_test)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-204-779a09789374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXGB_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \"\"\"\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, validate_features)\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \"\"\"\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0mtest_dmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mntree_limit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0mntree_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best_ntree_limit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_npy2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_dt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_init_from_npy2d\u001b[0;34m(self, mat, missing, nthread)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \"\"\"\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input numpy.ndarray must be 2 dimensional'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0;31m# flatten the array by rows and ensure it is float32.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;31m# we try to avoid data copies if possible (reshape returns a view when possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input numpy.ndarray must be 2 dimensional"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izFT5D5D_qV3"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7BkN_PbDuUi",
        "outputId": "a1e3f8fd-d9c0-4d56-f845-ff463c43f22f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256,input_shape=(20,7),activation='relu'))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_4 (LSTM)                (None, 256)               270336    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 336,385\n",
            "Trainable params: 336,385\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTFw5FgHDum1",
        "outputId": "dd57c0d2-b3de-4b5c-b86e-4241ab49034b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
        "mc = ModelCheckpoint('./best_model.h5', monitor='val_loss', mode='min',verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['acc'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=10, epochs=500, validation_data=(x_test, y_test) , callbacks=[es, mc])\n",
        "\n",
        "\n",
        "model.load_weights('best_model.h5')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6882 - acc: 0.5481\n",
            "Epoch 00001: val_loss improved from inf to 0.71937, saving model to ./best_model.h5\n",
            "48/48 [==============================] - 3s 61ms/step - loss: 0.6882 - acc: 0.5481 - val_loss: 0.7194 - val_acc: 0.3900\n",
            "Epoch 2/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6897 - acc: 0.5481\n",
            "Epoch 00002: val_loss did not improve from 0.71937\n",
            "48/48 [==============================] - 3s 55ms/step - loss: 0.6897 - acc: 0.5481 - val_loss: 0.7217 - val_acc: 0.3900\n",
            "Epoch 3/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6886 - acc: 0.5481\n",
            "Epoch 00003: val_loss improved from 0.71937 to 0.71659, saving model to ./best_model.h5\n",
            "48/48 [==============================] - 3s 56ms/step - loss: 0.6886 - acc: 0.5481 - val_loss: 0.7166 - val_acc: 0.3900\n",
            "Epoch 4/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6899 - acc: 0.5481\n",
            "Epoch 00004: val_loss improved from 0.71659 to 0.71560, saving model to ./best_model.h5\n",
            "48/48 [==============================] - 3s 57ms/step - loss: 0.6899 - acc: 0.5481 - val_loss: 0.7156 - val_acc: 0.3900\n",
            "Epoch 5/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6888 - acc: 0.5481\n",
            "Epoch 00005: val_loss did not improve from 0.71560\n",
            "48/48 [==============================] - 3s 57ms/step - loss: 0.6888 - acc: 0.5481 - val_loss: 0.7163 - val_acc: 0.3900\n",
            "Epoch 6/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6913 - acc: 0.5481\n",
            "Epoch 00006: val_loss did not improve from 0.71560\n",
            "48/48 [==============================] - 3s 55ms/step - loss: 0.6913 - acc: 0.5481 - val_loss: 0.7219 - val_acc: 0.3900\n",
            "Epoch 7/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6892 - acc: 0.5481\n",
            "Epoch 00007: val_loss did not improve from 0.71560\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6892 - acc: 0.5481 - val_loss: 0.7175 - val_acc: 0.3900\n",
            "Epoch 8/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6886 - acc: 0.5481\n",
            "Epoch 00008: val_loss did not improve from 0.71560\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6886 - acc: 0.5481 - val_loss: 0.7174 - val_acc: 0.3900\n",
            "Epoch 9/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6888 - acc: 0.5481\n",
            "Epoch 00009: val_loss did not improve from 0.71560\n",
            "48/48 [==============================] - 3s 55ms/step - loss: 0.6888 - acc: 0.5481 - val_loss: 0.7253 - val_acc: 0.3900\n",
            "Epoch 10/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6902 - acc: 0.5481\n",
            "Epoch 00010: val_loss did not improve from 0.71560\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6902 - acc: 0.5481 - val_loss: 0.7206 - val_acc: 0.3900\n",
            "Epoch 11/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6884 - acc: 0.5481\n",
            "Epoch 00011: val_loss did not improve from 0.71560\n",
            "48/48 [==============================] - 2s 52ms/step - loss: 0.6884 - acc: 0.5481 - val_loss: 0.7186 - val_acc: 0.3900\n",
            "Epoch 12/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6913 - acc: 0.5481\n",
            "Epoch 00012: val_loss improved from 0.71560 to 0.70995, saving model to ./best_model.h5\n",
            "48/48 [==============================] - 3s 55ms/step - loss: 0.6913 - acc: 0.5481 - val_loss: 0.7099 - val_acc: 0.3900\n",
            "Epoch 13/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6899 - acc: 0.5481\n",
            "Epoch 00013: val_loss did not improve from 0.70995\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6899 - acc: 0.5481 - val_loss: 0.7150 - val_acc: 0.3900\n",
            "Epoch 14/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6885 - acc: 0.5481\n",
            "Epoch 00014: val_loss did not improve from 0.70995\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6885 - acc: 0.5481 - val_loss: 0.7142 - val_acc: 0.3900\n",
            "Epoch 15/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6878 - acc: 0.5481\n",
            "Epoch 00015: val_loss did not improve from 0.70995\n",
            "48/48 [==============================] - 2s 51ms/step - loss: 0.6878 - acc: 0.5481 - val_loss: 0.7121 - val_acc: 0.3900\n",
            "Epoch 16/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6877 - acc: 0.5481\n",
            "Epoch 00016: val_loss did not improve from 0.70995\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6877 - acc: 0.5481 - val_loss: 0.7130 - val_acc: 0.3900\n",
            "Epoch 17/500\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6887 - acc: 0.5489\n",
            "Epoch 00017: val_loss did not improve from 0.70995\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6892 - acc: 0.5481 - val_loss: 0.7330 - val_acc: 0.3900\n",
            "Epoch 18/500\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6890 - acc: 0.5468\n",
            "Epoch 00018: val_loss did not improve from 0.70995\n",
            "48/48 [==============================] - 3s 52ms/step - loss: 0.6888 - acc: 0.5481 - val_loss: 0.7192 - val_acc: 0.3900\n",
            "Epoch 19/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6867 - acc: 0.5481\n",
            "Epoch 00019: val_loss did not improve from 0.70995\n",
            "48/48 [==============================] - 2s 52ms/step - loss: 0.6867 - acc: 0.5481 - val_loss: 0.7365 - val_acc: 0.3900\n",
            "Epoch 20/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6895 - acc: 0.5481\n",
            "Epoch 00020: val_loss improved from 0.70995 to 0.70711, saving model to ./best_model.h5\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6895 - acc: 0.5481 - val_loss: 0.7071 - val_acc: 0.3900\n",
            "Epoch 21/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6877 - acc: 0.5481\n",
            "Epoch 00021: val_loss did not improve from 0.70711\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6877 - acc: 0.5481 - val_loss: 0.7387 - val_acc: 0.3900\n",
            "Epoch 22/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6884 - acc: 0.5481\n",
            "Epoch 00022: val_loss did not improve from 0.70711\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6884 - acc: 0.5481 - val_loss: 0.7197 - val_acc: 0.3900\n",
            "Epoch 23/500\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6882 - acc: 0.5468\n",
            "Epoch 00023: val_loss did not improve from 0.70711\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6880 - acc: 0.5481 - val_loss: 0.7167 - val_acc: 0.3900\n",
            "Epoch 24/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6876 - acc: 0.5481\n",
            "Epoch 00024: val_loss did not improve from 0.70711\n",
            "48/48 [==============================] - 2s 51ms/step - loss: 0.6876 - acc: 0.5481 - val_loss: 0.7346 - val_acc: 0.3900\n",
            "Epoch 25/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6896 - acc: 0.5481\n",
            "Epoch 00025: val_loss did not improve from 0.70711\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6896 - acc: 0.5481 - val_loss: 0.7385 - val_acc: 0.3900\n",
            "Epoch 26/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6899 - acc: 0.5481\n",
            "Epoch 00026: val_loss improved from 0.70711 to 0.70501, saving model to ./best_model.h5\n",
            "48/48 [==============================] - 3s 56ms/step - loss: 0.6899 - acc: 0.5481 - val_loss: 0.7050 - val_acc: 0.3900\n",
            "Epoch 27/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6884 - acc: 0.5377\n",
            "Epoch 00027: val_loss did not improve from 0.70501\n",
            "48/48 [==============================] - 3s 55ms/step - loss: 0.6884 - acc: 0.5377 - val_loss: 0.7211 - val_acc: 0.3900\n",
            "Epoch 28/500\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6874 - acc: 0.5468\n",
            "Epoch 00028: val_loss did not improve from 0.70501\n",
            "48/48 [==============================] - 2s 51ms/step - loss: 0.6876 - acc: 0.5460 - val_loss: 0.7394 - val_acc: 0.3900\n",
            "Epoch 29/500\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6843 - acc: 0.5489\n",
            "Epoch 00029: val_loss improved from 0.70501 to 0.68869, saving model to ./best_model.h5\n",
            "48/48 [==============================] - 3s 57ms/step - loss: 0.6845 - acc: 0.5439 - val_loss: 0.6887 - val_acc: 0.5600\n",
            "Epoch 30/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6890 - acc: 0.5356\n",
            "Epoch 00030: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6890 - acc: 0.5356 - val_loss: 0.7103 - val_acc: 0.3900\n",
            "Epoch 31/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6854 - acc: 0.5460\n",
            "Epoch 00031: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6854 - acc: 0.5460 - val_loss: 0.7193 - val_acc: 0.3900\n",
            "Epoch 32/500\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6865 - acc: 0.5468\n",
            "Epoch 00032: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6869 - acc: 0.5439 - val_loss: 0.7330 - val_acc: 0.3900\n",
            "Epoch 33/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6877 - acc: 0.5439\n",
            "Epoch 00033: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 55ms/step - loss: 0.6877 - acc: 0.5439 - val_loss: 0.7496 - val_acc: 0.3900\n",
            "Epoch 34/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6875 - acc: 0.5377\n",
            "Epoch 00034: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 2s 51ms/step - loss: 0.6875 - acc: 0.5377 - val_loss: 0.7269 - val_acc: 0.3900\n",
            "Epoch 35/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6828 - acc: 0.5481\n",
            "Epoch 00035: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6828 - acc: 0.5481 - val_loss: 0.7237 - val_acc: 0.4700\n",
            "Epoch 36/500\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6860 - acc: 0.5511\n",
            "Epoch 00036: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 55ms/step - loss: 0.6867 - acc: 0.5502 - val_loss: 0.7521 - val_acc: 0.3900\n",
            "Epoch 37/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6845 - acc: 0.5418\n",
            "Epoch 00037: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 55ms/step - loss: 0.6845 - acc: 0.5418 - val_loss: 0.7928 - val_acc: 0.3900\n",
            "Epoch 38/500\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6916 - acc: 0.5447\n",
            "Epoch 00038: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6915 - acc: 0.5439 - val_loss: 0.7015 - val_acc: 0.4800\n",
            "Epoch 39/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6870 - acc: 0.5377\n",
            "Epoch 00039: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6870 - acc: 0.5377 - val_loss: 0.7128 - val_acc: 0.4700\n",
            "Epoch 40/500\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6862 - acc: 0.5383\n",
            "Epoch 00040: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6857 - acc: 0.5397 - val_loss: 0.7488 - val_acc: 0.3900\n",
            "Epoch 41/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6859 - acc: 0.5649\n",
            "Epoch 00041: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6859 - acc: 0.5649 - val_loss: 0.7302 - val_acc: 0.3900\n",
            "Epoch 42/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6853 - acc: 0.5481\n",
            "Epoch 00042: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6853 - acc: 0.5481 - val_loss: 0.7784 - val_acc: 0.3900\n",
            "Epoch 43/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6860 - acc: 0.5439\n",
            "Epoch 00043: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6860 - acc: 0.5439 - val_loss: 0.7274 - val_acc: 0.3700\n",
            "Epoch 44/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6869 - acc: 0.5523\n",
            "Epoch 00044: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6869 - acc: 0.5523 - val_loss: 0.7224 - val_acc: 0.4800\n",
            "Epoch 45/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6855 - acc: 0.5439\n",
            "Epoch 00045: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6855 - acc: 0.5439 - val_loss: 0.7545 - val_acc: 0.4600\n",
            "Epoch 46/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6843 - acc: 0.5628\n",
            "Epoch 00046: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 57ms/step - loss: 0.6843 - acc: 0.5628 - val_loss: 0.7454 - val_acc: 0.4800\n",
            "Epoch 47/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6809 - acc: 0.5565\n",
            "Epoch 00047: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 56ms/step - loss: 0.6809 - acc: 0.5565 - val_loss: 0.8122 - val_acc: 0.3900\n",
            "Epoch 48/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6829 - acc: 0.5439\n",
            "Epoch 00048: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6829 - acc: 0.5439 - val_loss: 0.8387 - val_acc: 0.4800\n",
            "Epoch 49/500\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6833 - acc: 0.5511\n",
            "Epoch 00049: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 56ms/step - loss: 0.6839 - acc: 0.5481 - val_loss: 0.7977 - val_acc: 0.4700\n",
            "Epoch 50/500\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6858 - acc: 0.5660\n",
            "Epoch 00050: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6854 - acc: 0.5690 - val_loss: 0.8758 - val_acc: 0.3900\n",
            "Epoch 51/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6793 - acc: 0.5586\n",
            "Epoch 00051: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6793 - acc: 0.5586 - val_loss: 1.1670 - val_acc: 0.3900\n",
            "Epoch 52/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6865 - acc: 0.5418\n",
            "Epoch 00052: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6865 - acc: 0.5418 - val_loss: 0.9191 - val_acc: 0.4700\n",
            "Epoch 53/500\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6836 - acc: 0.5447\n",
            "Epoch 00053: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 2s 52ms/step - loss: 0.6836 - acc: 0.5439 - val_loss: 0.7661 - val_acc: 0.3900\n",
            "Epoch 54/500\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6845 - acc: 0.5468\n",
            "Epoch 00054: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6847 - acc: 0.5502 - val_loss: 0.8645 - val_acc: 0.3900\n",
            "Epoch 55/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6825 - acc: 0.5502\n",
            "Epoch 00055: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6825 - acc: 0.5502 - val_loss: 1.1101 - val_acc: 0.3900\n",
            "Epoch 56/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6809 - acc: 0.5460\n",
            "Epoch 00056: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 55ms/step - loss: 0.6809 - acc: 0.5460 - val_loss: 0.9783 - val_acc: 0.3900\n",
            "Epoch 57/500\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6810 - acc: 0.5335\n",
            "Epoch 00057: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6810 - acc: 0.5335 - val_loss: 0.8272 - val_acc: 0.5300\n",
            "Epoch 58/500\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6838 - acc: 0.5404\n",
            "Epoch 00058: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 53ms/step - loss: 0.6826 - acc: 0.5377 - val_loss: 0.9580 - val_acc: 0.3900\n",
            "Epoch 59/500\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6775 - acc: 0.5745\n",
            "Epoch 00059: val_loss did not improve from 0.68869\n",
            "48/48 [==============================] - 3s 54ms/step - loss: 0.6787 - acc: 0.5669 - val_loss: 0.8814 - val_acc: 0.5100\n",
            "Epoch 00059: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-kMlruHESXX",
        "outputId": "72ecbc91-e460-40c0-c677-d1d713a3396c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "x_train"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[9.59742351e-01, 5.76841265e-04, 5.78778135e-02, ...,\n",
              "         9.56493078e-01, 9.10790598e-02, 6.82170935e-03],\n",
              "        [9.39613527e-01, 0.00000000e+00, 5.92558567e-02, ...,\n",
              "         9.48582729e-01, 9.05448718e-02, 6.13791032e-03],\n",
              "        [9.77455717e-01, 6.48946423e-04, 3.95039045e-02, ...,\n",
              "         9.26829268e-01, 7.98611111e-02, 2.87503121e-02],\n",
              "        ...,\n",
              "        [9.56119163e-01, 0.00000000e+00, 5.83371612e-02, ...,\n",
              "         9.49241925e-01, 6.25000000e-02, 2.61833522e-02],\n",
              "        [9.73027375e-01, 0.00000000e+00, 5.32843362e-02, ...,\n",
              "         9.20237310e-01, 6.65064103e-02, 4.31806423e-03],\n",
              "        [9.68196457e-01, 0.00000000e+00, 4.59347726e-02, ...,\n",
              "         9.24851681e-01, 6.91773504e-02, 2.18797599e-02]],\n",
              "\n",
              "       [[9.39613527e-01, 0.00000000e+00, 5.92558567e-02, ...,\n",
              "         9.48582729e-01, 9.05448718e-02, 6.13791032e-03],\n",
              "        [9.77455717e-01, 6.48946423e-04, 3.95039045e-02, ...,\n",
              "         9.26829268e-01, 7.98611111e-02, 2.87503121e-02],\n",
              "        [1.00000000e+00, 3.84560844e-04, 0.00000000e+00, ...,\n",
              "         9.36717205e-01, 7.50534188e-02, 3.99769173e-02],\n",
              "        ...,\n",
              "        [9.73027375e-01, 0.00000000e+00, 5.32843362e-02, ...,\n",
              "         9.20237310e-01, 6.65064103e-02, 4.31806423e-03],\n",
              "        [9.68196457e-01, 0.00000000e+00, 4.59347726e-02, ...,\n",
              "         9.24851681e-01, 6.91773504e-02, 2.18797599e-02],\n",
              "        [9.74235105e-01, 0.00000000e+00, 2.84795590e-02, ...,\n",
              "         9.31443639e-01, 6.14316239e-02, 3.36128829e-02]],\n",
              "\n",
              "       [[9.77455717e-01, 6.48946423e-04, 3.95039045e-02, ...,\n",
              "         9.26829268e-01, 7.98611111e-02, 2.87503121e-02],\n",
              "        [1.00000000e+00, 3.84560844e-04, 0.00000000e+00, ...,\n",
              "         9.36717205e-01, 7.50534188e-02, 3.99769173e-02],\n",
              "        [9.87520129e-01, 4.13402907e-04, 3.72071658e-02, ...,\n",
              "         9.50560316e-01, 7.39850427e-02, 3.09337656e-02],\n",
              "        ...,\n",
              "        [9.68196457e-01, 0.00000000e+00, 4.59347726e-02, ...,\n",
              "         9.24851681e-01, 6.91773504e-02, 2.18797599e-02],\n",
              "        [9.74235105e-01, 0.00000000e+00, 2.84795590e-02, ...,\n",
              "         9.31443639e-01, 6.14316239e-02, 3.36128829e-02],\n",
              "        [9.81078905e-01, 0.00000000e+00, 4.04225999e-02, ...,\n",
              "         9.18918919e-01, 6.57051282e-02, 8.73019606e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[2.48389694e-01, 6.04000875e-03, 7.36334405e-01, ...,\n",
              "         1.74027686e-01, 5.34188034e-01, 3.06258751e-01],\n",
              "        [2.48389694e-01, 8.04453215e-03, 7.48277446e-01, ...,\n",
              "         1.78642057e-01, 5.56623932e-01, 1.53187263e-01],\n",
              "        [3.20853462e-01, 2.26890898e-03, 7.40927882e-01, ...,\n",
              "         1.69413316e-01, 5.64102564e-01, 1.15180338e-01],\n",
              "        ...,\n",
              "        [4.46054750e-01, 7.21051582e-06, 7.22094626e-01, ...,\n",
              "         1.56888596e-01, 6.53311966e-01, 2.31160251e-01],\n",
              "        [4.46054750e-01, 0.00000000e+00, 6.89940285e-01, ...,\n",
              "         1.39090310e-01, 6.62126068e-01, 1.04519948e-01],\n",
              "        [4.04589372e-01, 8.32333876e-03, 7.40009187e-01, ...,\n",
              "         1.18655241e-01, 6.63194444e-01, 1.00948997e-01]],\n",
              "\n",
              "       [[2.48389694e-01, 8.04453215e-03, 7.48277446e-01, ...,\n",
              "         1.78642057e-01, 5.56623932e-01, 1.53187263e-01],\n",
              "        [3.20853462e-01, 2.26890898e-03, 7.40927882e-01, ...,\n",
              "         1.69413316e-01, 5.64102564e-01, 1.15180338e-01],\n",
              "        [3.56280193e-01, 3.77350328e-03, 7.03720717e-01, ...,\n",
              "         1.66117337e-01, 5.92681624e-01, 2.35212394e-01],\n",
              "        ...,\n",
              "        [4.46054750e-01, 0.00000000e+00, 6.89940285e-01, ...,\n",
              "         1.39090310e-01, 6.62126068e-01, 1.04519948e-01],\n",
              "        [4.04589372e-01, 8.32333876e-03, 7.40009187e-01, ...,\n",
              "         1.18655241e-01, 6.63194444e-01, 1.00948997e-01],\n",
              "        [4.32769726e-01, 3.12455685e-05, 7.07854846e-01, ...,\n",
              "         1.08767304e-01, 6.58119658e-01, 1.15256316e-01]],\n",
              "\n",
              "       [[3.20853462e-01, 2.26890898e-03, 7.40927882e-01, ...,\n",
              "         1.69413316e-01, 5.64102564e-01, 1.15180338e-01],\n",
              "        [3.56280193e-01, 3.77350328e-03, 7.03720717e-01, ...,\n",
              "         1.66117337e-01, 5.92681624e-01, 2.35212394e-01],\n",
              "        [3.24074074e-01, 1.45412069e-03, 7.02342673e-01, ...,\n",
              "         1.50955834e-01, 5.81997863e-01, 9.35502194e-02],\n",
              "        ...,\n",
              "        [4.04589372e-01, 8.32333876e-03, 7.40009187e-01, ...,\n",
              "         1.18655241e-01, 6.63194444e-01, 1.00948997e-01],\n",
              "        [4.32769726e-01, 3.12455685e-05, 7.07854846e-01, ...,\n",
              "         1.08767304e-01, 6.58119658e-01, 1.15256316e-01],\n",
              "        [4.28341385e-01, 7.21051582e-06, 7.21175930e-01, ...,\n",
              "         9.95385630e-02, 6.44497863e-01, 2.04323853e-01]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBmr6kJbGP_3",
        "outputId": "804f04ee-746e-49f9-8947-b9de7b36b2d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "y_train"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nh7sKbbGQwW",
        "outputId": "4120260d-ecdc-40b0-b3e0-2f3c3c776229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "x_test"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[3.56280193e-01, 3.77350328e-03, 7.03720717e-01, ...,\n",
              "         1.66117337e-01, 5.92681624e-01, 2.35212394e-01],\n",
              "        [3.24074074e-01, 1.45412069e-03, 7.02342673e-01, ...,\n",
              "         1.50955834e-01, 5.81997863e-01, 9.35502194e-02],\n",
              "        [3.59500805e-01, 2.40831228e-03, 6.95911805e-01, ...,\n",
              "         1.25906394e-01, 5.81997863e-01, 9.80654638e-02],\n",
              "        ...,\n",
              "        [4.32769726e-01, 3.12455685e-05, 7.07854846e-01, ...,\n",
              "         1.08767304e-01, 6.58119658e-01, 1.15256316e-01],\n",
              "        [4.28341385e-01, 7.21051582e-06, 7.21175930e-01, ...,\n",
              "         9.95385630e-02, 6.44497863e-01, 2.04323853e-01],\n",
              "        [4.56924316e-01, 1.69927823e-03, 6.97749196e-01, ...,\n",
              "         7.84442980e-02, 6.61324786e-01, 8.93370767e-02]],\n",
              "\n",
              "       [[3.24074074e-01, 1.45412069e-03, 7.02342673e-01, ...,\n",
              "         1.50955834e-01, 5.81997863e-01, 9.35502194e-02],\n",
              "        [3.59500805e-01, 2.40831228e-03, 6.95911805e-01, ...,\n",
              "         1.25906394e-01, 5.81997863e-01, 9.80654638e-02],\n",
              "        [3.59500805e-01, 1.44210316e-03, 6.46761599e-01, ...,\n",
              "         1.15359262e-01, 6.05769231e-01, 2.03958437e-01],\n",
              "        ...,\n",
              "        [4.28341385e-01, 7.21051582e-06, 7.21175930e-01, ...,\n",
              "         9.95385630e-02, 6.44497863e-01, 2.04323853e-01],\n",
              "        [4.56924316e-01, 1.69927823e-03, 6.97749196e-01, ...,\n",
              "         7.84442980e-02, 6.61324786e-01, 8.93370767e-02],\n",
              "        [4.34380032e-01, 5.06418561e-03, 7.00505282e-01, ...,\n",
              "         8.63546473e-02, 6.55448718e-01, 8.98707294e-02]],\n",
              "\n",
              "       [[3.59500805e-01, 2.40831228e-03, 6.95911805e-01, ...,\n",
              "         1.25906394e-01, 5.81997863e-01, 9.80654638e-02],\n",
              "        [3.59500805e-01, 1.44210316e-03, 6.46761599e-01, ...,\n",
              "         1.15359262e-01, 6.05769231e-01, 2.03958437e-01],\n",
              "        [3.38164251e-01, 1.21136666e-03, 6.88562242e-01, ...,\n",
              "         1.32498352e-01, 6.27403846e-01, 2.67544510e-01],\n",
              "        ...,\n",
              "        [4.56924316e-01, 1.69927823e-03, 6.97749196e-01, ...,\n",
              "         7.84442980e-02, 6.61324786e-01, 8.93370767e-02],\n",
              "        [4.34380032e-01, 5.06418561e-03, 7.00505282e-01, ...,\n",
              "         8.63546473e-02, 6.55448718e-01, 8.98707294e-02],\n",
              "        [4.30756844e-01, 2.41311929e-03, 7.20257235e-01, ...,\n",
              "         5.99868161e-02, 6.57585470e-01, 8.19455349e-02]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[8.94927536e-01, 6.29718381e-03, 5.37436840e-01, ...,\n",
              "         3.75741595e-02, 8.36271368e-01, 1.76143373e-01],\n",
              "        [9.13043478e-01, 3.23511810e-03, 5.34221406e-01, ...,\n",
              "         6.46011866e-02, 8.37072650e-01, 1.84593176e-01],\n",
              "        [8.62318841e-01, 8.65261898e-03, 5.50298576e-01, ...,\n",
              "         9.55833883e-02, 8.63782051e-01, 1.61966302e-01],\n",
              "        ...,\n",
              "        [9.35587762e-01, 6.46035779e-01, 4.13412954e-01, ...,\n",
              "         7.18523401e-02, 8.03952991e-01, 1.26439505e-01],\n",
              "        [8.71175523e-01, 8.21037401e-01, 4.45567294e-01, ...,\n",
              "         6.72379697e-02, 7.74038462e-01, 2.16713640e-01],\n",
              "        [8.86070853e-01, 1.00000000e+00, 4.42811208e-01, ...,\n",
              "         5.80092287e-02, 7.38247863e-01, 2.38296725e-01]],\n",
              "\n",
              "       [[9.13043478e-01, 3.23511810e-03, 5.34221406e-01, ...,\n",
              "         6.46011866e-02, 8.37072650e-01, 1.84593176e-01],\n",
              "        [8.62318841e-01, 8.65261898e-03, 5.50298576e-01, ...,\n",
              "         9.55833883e-02, 8.63782051e-01, 1.61966302e-01],\n",
              "        [8.91304348e-01, 6.73702528e-03, 5.28249885e-01, ...,\n",
              "         1.19973632e-01, 8.80341880e-01, 1.39683137e-01],\n",
              "        ...,\n",
              "        [8.71175523e-01, 8.21037401e-01, 4.45567294e-01, ...,\n",
              "         6.72379697e-02, 7.74038462e-01, 2.16713640e-01],\n",
              "        [8.86070853e-01, 1.00000000e+00, 4.42811208e-01, ...,\n",
              "         5.80092287e-02, 7.38247863e-01, 2.38296725e-01],\n",
              "        [8.22866345e-01, 9.19160504e-01, 4.80937069e-01, ...,\n",
              "         4.08701384e-02, 7.24893162e-01, 3.53697761e-01]],\n",
              "\n",
              "       [[8.62318841e-01, 8.65261898e-03, 5.50298576e-01, ...,\n",
              "         9.55833883e-02, 8.63782051e-01, 1.61966302e-01],\n",
              "        [8.91304348e-01, 6.73702528e-03, 5.28249885e-01, ...,\n",
              "         1.19973632e-01, 8.80341880e-01, 1.39683137e-01],\n",
              "        [9.01771337e-01, 4.47532682e-03, 5.39274231e-01, ...,\n",
              "         8.43770600e-02, 8.61912393e-01, 9.89844318e-02],\n",
              "        ...,\n",
              "        [8.86070853e-01, 1.00000000e+00, 4.42811208e-01, ...,\n",
              "         5.80092287e-02, 7.38247863e-01, 2.38296725e-01],\n",
              "        [8.22866345e-01, 9.19160504e-01, 4.80937069e-01, ...,\n",
              "         4.08701384e-02, 7.24893162e-01, 3.53697761e-01],\n",
              "        [8.38566828e-01, 6.47879267e-01, 4.79099678e-01, ...,\n",
              "         3.95517469e-02, 7.44658120e-01, 1.34393644e-01]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST-Fs6WhGRq2",
        "outputId": "3f68518e-ca7d-43fc-fbab-1466b57da42b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "y_test"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsxGr7htGSUi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}