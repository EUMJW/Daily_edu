# -*- coding: utf-8 -*-
"""자연어.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ptjHNXHFrKI0sOXhsfUfKCezBfByBGZO
"""

import json
import tensorflow as tf
import numpy as np
import urllib
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas as pd
from nltk import word_tokenize

def solution_model():
    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'
    urllib.request.urlretrieve(url, 'sarcasm.json')

    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK
    vocab_size = 1000
    embedding_dim = 16
    max_length = 120
    trunc_type='post'
    padding_type='post'
    oov_tok = "<OOV>"
    training_size = 20000

    sentences = []
    labels = []
    # YOUR CODE HERE



    model = tf.keras.Sequential([
    # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    return model


# Note that you'll need to save your model as a .h5 like this.
# When you press the Submit and Test button, your saved .h5 model will
# be sent to the testing infrastructure for scoring
# and the score will be returned to you.
if __name__ == '__main__':
    model = solution_model()
    model.save("mymodel.h5")

url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'
urllib.request.urlretrieve(url, 'sarcasm.json')

import pandas as pd

data = pd.read_json('sarcasm.json')

data

x_data = [word_tokenize(i) for i in data['headline']]
y_data = data['is_sarcastic']

word_list = []
for text_list in x_data:
  for word in text_list:
    word_list.append(word)

len(word_list)

word_list = list(set(word_list))

len(word_list)

oov_tok =["<OOV>"]
word_list[:0] = oov_tok

word_index = {word : index for index, word in enumerate(word_list)}

word_index

def text_to_index(sentences, word_list): 
    sentences_index = []
    
    for sentence in sentences:
        sentence_index = []
        
        for word in sentence:
            if word_list.get(word) is not None:
                sentence_index.extend([word_list[word]])
            else:
                sentence_index.extend([word_list[OOV]])

        if len(sentence_index) > max_length:
            sentence_index = sentence_index[:max_length]
            
        sentence_index += (max_length - len(sentence_index)) * ['0']
        
        sentences_index.append(sentence_index)

    return np.asarray(sentences_index)

max_length = 120

x_train = text_to_index(x_data,word_index).astype('float32')
y_train = np.array(y_data).astype('float32')

print(x_train.shape, y_train.shape)

from keras import models
from keras import layers
from keras import optimizers, losses, metrics

embedding_dim = 16


model = models.Sequential()
model.add(layers.Embedding(len(word_list), embedding_dim, input_length=max_sequences))
model.add(layers.Dropout(0.2))
model.add(layers.Conv1D(64, 8, activation='relu'))
model.add(layers.MaxPooling1D(5))
model.add(layers.Conv1D(64, 8, activation='relu'))
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.2))
model.add(layers.Dense(1, activation='sigmoid'))

model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
model.fit(x_train, y_train, epochs=20)